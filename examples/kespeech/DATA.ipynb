{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理KeSpeech-ASR数据集\n",
    "\n",
    "|- Each_subdialect    \n",
    "&emsp;|- Mandarin   \n",
    "&emsp;|- BJ   \n",
    "&emsp;|- SW   \n",
    "&emsp;|- ZY    \n",
    "&emsp;|- NE   \n",
    "&emsp;|- LY  \n",
    "&emsp;|- JH    \n",
    "&emsp;|- JLu   \n",
    "&emsp;|- JLo   \n",
    "|- All_subdialect   \n",
    "|- Mandarin  \n",
    "|-Whole_training_set  \n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T02:56:17.486326Z",
     "start_time": "2024-08-30T02:56:17.481085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "kespeech office data\n",
    "ASR:                                            \n",
    "    train:\n",
    "        P1: 543633\n",
    "        P2: 340387       Total: 884020\n",
    "    dev:\n",
    "        P1: 2199\n",
    "        P2: 2207         Total: 4406\n",
    "    test:               Total: 19723\n",
    "AR:\n",
    "    train: 543633\n",
    "    dev: 2199\n",
    "    test: 44306\n",
    "    \n",
    "data1 为原始ASR数据  为phase1+phase2 data\n",
    "ASR:                                            \n",
    "    train audio_datasets.jsonl: 881808\n",
    "    dev audio_datasets.jsonl: 16883  注意验证数据中可能包含有test数据\n",
    "    test audio_datasets.jsonl: 19668\n",
    "    \n",
    "data2 为原始ASR数据加上口音信息  增加了text_language字段  注意验证数据中可能包含有test数据\n",
    "\n",
    "data3 在data2的基础上增加了kmeans伪标签 同时生成了单独的phase1数据audio_datasets_phase1.jsonl  注意验证数据中可能包含有test数据\n",
    "ASR:                                            \n",
    "    train audio_datasets.jsonl: 881808\n",
    "    dev audio_datasets.jsonl: 16883  注意验证数据中可能包含有test数据\n",
    "    test audio_datasets.jsonl: 19668\n",
    "    \n",
    "    train audio_datasets_phase1.jsonl: 542697\n",
    "    dev audio_datasets_phase1.jsonl: 16833\n",
    "    test audio_datasets_phase1.jsonl: 16727\n",
    "    \n",
    "data4 为data2的基础上把所有不含phase1对应的text_language设置成None  注意验证数据中可能包含有test数据\n",
    "\n",
    "data5 只有phase1的内容 kespeech的默认ar数据集\n",
    "AR:\n",
    "    train: 542697\n",
    "    dev: 2196\n",
    "    test: 16727\n",
    "\n",
    "data6 标准的asr phase1+phase2数据集  test数据已从所有dev数据集中删除  P2的口音label为Mandarin\n",
    "ASR:\n",
    "    train audio_datasets.jsonl: 881808\n",
    "    dev audio_datasets.jsonl: 4399\n",
    "    test audio_datasets.jsonl: 19668\n",
    "'''\n",
    "\n",
    "\n",
    "# data1 为原始ASR数据  为phase1+phase2 data 注意验证数据中可能包含有test数据\n",
    "# data2 为原始ASR数据加上口音信息  增加了text_language字段  注意验证数据中可能包含有test数据\n",
    "# data3 在data2的基础上增加了kmeans伪标签 同时生成了单独的phase1数据audio_datasets_phase1.jsonl  注意验证数据中可能包含有test数据\n",
    "# data4 为data2的基础上把所有不含phase1对应的text_language设置成None  注意验证数据中可能包含有test数据\n",
    "# data5 只有phase1的内容 kespeech的默认ar数据集\n",
    "# data6 标准的asr phase1+phase2数据集  test数据已从所有dev数据集中删除"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "root_dir = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech\"\n",
    "audio_dir = os.path.join(root_dir, \"Audio\")\n",
    "asr_text_dir = os.path.join(root_dir, \"Tasks/ASR\")\n",
    "train_phase_1_dir = os.path.join(asr_text_dir, \"train_phase1\")\n",
    "train_phase_2_dir = os.path.join(asr_text_dir, \"train_phase2\")\n",
    "train_list = [train_phase_1_dir, train_phase_2_dir]\n",
    "dev_phase_1_dir = os.path.join(asr_text_dir, \"dev_phase1\")\n",
    "dev_phase_2_dir = os.path.join(asr_text_dir, \"dev_phase2\")\n",
    "dev_list = [dev_phase_1_dir, dev_phase_2_dir]\n",
    "test_dir = os.path.join(asr_text_dir, \"test\")\n",
    "\n",
    "output_dir = \"/ssd/zhuang/code/FunASR2024/examples/kespeech/DATA/data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_wav_scp(wav_scp, dict):\n",
    "    with open(wav_scp, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, path = line.split(\" \")\n",
    "        path = os.path.join(root_dir, path)\n",
    "        if os.path.exists(path):\n",
    "            dict[id] = {}\n",
    "            dict[id].update({\"path\": path})\n",
    "    return dict\n",
    "\n",
    "def gether_dialect_info(utt2subdialect, dict):\n",
    "    with open(utt2subdialect, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, dialect = line.split(\" \")\n",
    "        dict[id].update({\"dialect\": dialect})\n",
    "    return dict\n",
    "\n",
    "def gather_text_info(text, dict):\n",
    "    with open(text, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, text = line.split(\" \", 1)\n",
    "        dict[id].update({\"text\": text})\n",
    "    return dict\n",
    "\n",
    "def contains_non_chinese(text):\n",
    "    # 正则表达式匹配非中文字符\n",
    "    non_chinese_pattern = re.compile(r'[^\\u4e00-\\u9fff]')\n",
    "    return non_chinese_pattern.search(text) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Train dataset size:  881808\n",
      "Dev dataset size:  4399\n",
      "Test dataset size:  19668\n"
     ]
    }
   ],
   "source": [
    "train_dict  = {}\n",
    "dev_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "for train_path in train_list:\n",
    "    train_wav_scp = os.path.join(train_path, \"wav.scp\")\n",
    "    train_utts2subdialect = os.path.join(train_path, \"utt2subdialect\")\n",
    "    traain_text = os.path.join(train_path, \"text\")\n",
    "    train_dict = gather_wav_scp(train_wav_scp, train_dict)\n",
    "    train_dict = gether_dialect_info(train_utts2subdialect, train_dict)\n",
    "    train_dict = gather_text_info(traain_text, train_dict)\n",
    "for dev_path in dev_list:\n",
    "    dev_wav_scp = os.path.join(dev_path, \"wav.scp\")\n",
    "    dev_utts2subdialect = os.path.join(dev_path, \"utt2subdialect\")\n",
    "    dev_text = os.path.join(dev_path, \"text\")\n",
    "    dev_dict = gather_wav_scp(dev_wav_scp, dev_dict)\n",
    "    dev_dict = gether_dialect_info(dev_utts2subdialect, dev_dict)\n",
    "    dev_dict = gather_text_info(dev_text, dev_dict)\n",
    "\n",
    "test_wav_scp = os.path.join(test_dir, \"wav.scp\")\n",
    "test_utts2subdialect = os.path.join(test_dir, \"utt2subdialect\")\n",
    "test_text = os.path.join(test_dir, \"text\")\n",
    "test_dict = gather_wav_scp(test_wav_scp, test_dict)\n",
    "test_dict = gether_dialect_info(test_utts2subdialect, test_dict)\n",
    "test_dict = gather_text_info(test_text, test_dict)\n",
    "\n",
    "clean_train_dict = {}\n",
    "clean_dev_dict = {}\n",
    "clean_test_dict = {}\n",
    "\n",
    "for id in train_dict.keys():\n",
    "    if contains_non_chinese(train_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_train_dict[id] = train_dict[id]\n",
    "for id in dev_dict.keys():\n",
    "    if contains_non_chinese(dev_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_dev_dict[id] = dev_dict[id]\n",
    "for id in test_dict.keys():\n",
    "    if contains_non_chinese(test_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_test_dict[id] = test_dict[id]\n",
    "\n",
    "train_dict = clean_train_dict\n",
    "dev_dict = clean_dev_dict\n",
    "test_dict = clean_test_dict\n",
    "\n",
    "\n",
    "print (\"Done!\")\n",
    "print (\"Train dataset size: \", len(train_dict))\n",
    "print (\"Dev dataset size: \", len(dev_dict))\n",
    "print (\"Test dataset size: \", len(test_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each_subdialect   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_subdialect_dir =  os.path.join(output_dir, \"ES\")\n",
    "os.makedirs(each_subdialect_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Make wav.scp and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin train size:  678515 check True\n",
      "Mandarin dev size:  3681 check True\n",
      "Mandarin test size:  4981 check True\n",
      "Done!\n",
      "Beijing train size:  2237 check True\n",
      "Beijing dev size:  31 check True\n",
      "Beijing test size:  265 check True\n",
      "Done!\n",
      "Southwestern train size:  45359 check True\n",
      "Southwestern dev size:  132 check True\n",
      "Southwestern test size:  2684 check True\n",
      "Done!\n",
      "Jiao-Liao train size:  20268 check True\n",
      "Jiao-Liao dev size:  118 check True\n",
      "Jiao-Liao test size:  1443 check True\n",
      "Done!\n",
      "Northeastern train size:  4843 check True\n",
      "Northeastern dev size:  5 check True\n",
      "Northeastern test size:  350 check True\n",
      "Done!\n",
      "Jiang-Huai train size:  27586 check True\n",
      "Jiang-Huai dev size:  105 check True\n",
      "Jiang-Huai test size:  2268 check True\n",
      "Done!\n",
      "Lan-Yin train size:  20549 check True\n",
      "Lan-Yin dev size:  104 check True\n",
      "Lan-Yin test size:  1646 check True\n",
      "Done!\n",
      "Ji-Lu train size:  33861 check True\n",
      "Ji-Lu dev size:  156 check True\n",
      "Ji-Lu test size:  2806 check True\n",
      "Done!\n",
      "Zhongyuan train size:  48590 check True\n",
      "Zhongyuan dev size:  67 check True\n",
      "Zhongyuan test size:  3225 check True\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Mandarin', 'Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "for dialect in subdialect_list:\n",
    "    es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "    os.makedirs(es_subdialect_dir, exist_ok=True)\n",
    "\n",
    "    es_subdialect_train = os.path.join(es_subdialect_dir, \"train\")\n",
    "    es_subdialect_dev = os.path.join(es_subdialect_dir, \"dev\")\n",
    "    es_subdialect_test = os.path.join(es_subdialect_dir, \"test\")\n",
    "\n",
    "    es_subdialect_output = [es_subdialect_train, es_subdialect_dev, es_subdialect_test]\n",
    "    data_dict = [train_dict, dev_dict, test_dict]\n",
    "\n",
    "    for output, dict in zip(es_subdialect_output, data_dict):\n",
    "        os.makedirs(output, exist_ok=True)\n",
    "        wav_scp = os.path.join(output, \"wav.scp\")\n",
    "        text_path = os.path.join(output, \"text\")\n",
    "\n",
    "        with open(wav_scp, 'w') as f:\n",
    "            for id, info in dict.items():\n",
    "                if info[\"dialect\"] == dialect:\n",
    "                    path = info[\"path\"]\n",
    "                    f.write(f\"{id} {path}\\n\")\n",
    "\n",
    "        with open(text_path, 'w') as f:\n",
    "            for id, info in dict.items():\n",
    "                if info[\"dialect\"] == dialect:\n",
    "                    text = info[\"text\"]\n",
    "                    f.write(f\"{id} {text}\\n\")\n",
    "\n",
    "        print (dialect, output.split(\"/\")[-1], \"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))\n",
    "            \n",
    "    print (\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the test to the dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(read_file, writen_file):\n",
    "    # 打开第一个文件以读取数据\n",
    "    with open(read_file, 'r', encoding='utf-8') as f1:\n",
    "        data = f1.read()  # 读取全部内容\n",
    "\n",
    "    # 打开第二个文件以追加数据\n",
    "    with open(writen_file, 'a', encoding='utf-8') as f2:\n",
    "        f2.write(data)  # 将读取的数据追加到文件末尾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing Dev dataset oringinal size:  31 = 31\n",
      "Beijing Test dataset oringinal size:  265 = 265\n",
      "Beijing Dev dataset size:  296 = 296\n",
      "Beijing Test dataset size:  265 = 265\n",
      "\n",
      "\n",
      "Southwestern Dev dataset oringinal size:  132 = 132\n",
      "Southwestern Test dataset oringinal size:  2684 = 2684\n",
      "Southwestern Dev dataset size:  2816 = 2816\n",
      "Southwestern Test dataset size:  2684 = 2684\n",
      "\n",
      "\n",
      "Jiao-Liao Dev dataset oringinal size:  118 = 118\n",
      "Jiao-Liao Test dataset oringinal size:  1443 = 1443\n",
      "Jiao-Liao Dev dataset size:  1561 = 1561\n",
      "Jiao-Liao Test dataset size:  1443 = 1443\n",
      "\n",
      "\n",
      "Northeastern Dev dataset oringinal size:  5 = 5\n",
      "Northeastern Test dataset oringinal size:  350 = 350\n",
      "Northeastern Dev dataset size:  355 = 355\n",
      "Northeastern Test dataset size:  350 = 350\n",
      "\n",
      "\n",
      "Jiang-Huai Dev dataset oringinal size:  105 = 105\n",
      "Jiang-Huai Test dataset oringinal size:  2268 = 2268\n",
      "Jiang-Huai Dev dataset size:  2373 = 2373\n",
      "Jiang-Huai Test dataset size:  2268 = 2268\n",
      "\n",
      "\n",
      "Lan-Yin Dev dataset oringinal size:  104 = 104\n",
      "Lan-Yin Test dataset oringinal size:  1646 = 1646\n",
      "Lan-Yin Dev dataset size:  1750 = 1750\n",
      "Lan-Yin Test dataset size:  1646 = 1646\n",
      "\n",
      "\n",
      "Ji-Lu Dev dataset oringinal size:  156 = 156\n",
      "Ji-Lu Test dataset oringinal size:  2806 = 2806\n",
      "Ji-Lu Dev dataset size:  2962 = 2962\n",
      "Ji-Lu Test dataset size:  2806 = 2806\n",
      "\n",
      "\n",
      "Zhongyuan Dev dataset oringinal size:  67 = 67\n",
      "Zhongyuan Test dataset oringinal size:  3225 = 3225\n",
      "Zhongyuan Dev dataset size:  3292 = 3292\n",
      "Zhongyuan Test dataset size:  3225 = 3225\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "for dialect in subdialect_list:\n",
    "    \n",
    "    es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "    es_subdialect_dev = os.path.join(es_subdialect_dir, \"dev\")\n",
    "    es_subdialect_test = os.path.join(es_subdialect_dir, \"test\")\n",
    "\n",
    "    es_subdialect_dev_wav_scp = os.path.join(es_subdialect_dev, \"wav.scp\")\n",
    "    es_subdialect_dev_text = os.path.join(es_subdialect_dev, \"text\")\n",
    "\n",
    "    es_subdialect_test_wav_scp = os.path.join(es_subdialect_test, \"wav.scp\")\n",
    "    es_subdialect_test_text = os.path.join(es_subdialect_test, \"text\")\n",
    "\n",
    "    print (dialect, \"Dev dataset oringinal size: \", len(open(es_subdialect_dev_wav_scp).readlines()), \"=\", len(open(es_subdialect_dev_text).readlines()))\n",
    "    print (dialect, \"Test dataset oringinal size: \", len(open(es_subdialect_test_wav_scp).readlines()), \"=\", len(open(es_subdialect_test_text).readlines()))\n",
    "\n",
    "    merge_files(es_subdialect_test_wav_scp, es_subdialect_dev_wav_scp)\n",
    "    merge_files(es_subdialect_test_text, es_subdialect_dev_text)\n",
    "\n",
    "    print (dialect, \"Dev dataset size: \", len(open(es_subdialect_dev_wav_scp).readlines()), \"=\", len(open(es_subdialect_dev_text).readlines()))\n",
    "    print (dialect, \"Test dataset size: \", len(open(es_subdialect_test_wav_scp).readlines()),\"=\", len(open(es_subdialect_test_text).readlines()))\n",
    "\n",
    "    print (\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All_subdialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  203293 check True\n",
      "dev size:  15405 check True\n",
      "test size:  14687 check True\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "\n",
    "all_subdialect_dir = os.path.join(output_dir, \"AS\")\n",
    "os.makedirs(all_subdialect_dir, exist_ok=True)\n",
    "all_subdialect_train = os.path.join(all_subdialect_dir, \"train\")\n",
    "WholeDevSet = os.path.join(all_subdialect_dir, \"dev\")\n",
    "WholeTestSet = os.path.join(all_subdialect_dir, \"test\")\n",
    "whole_dataset_output = [all_subdialect_train, WholeDevSet, WholeTestSet]\n",
    "\n",
    "for output in whole_dataset_output:\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    wav_scp = os.path.join(output, \"wav.scp\")\n",
    "    text_path = os.path.join(output, \"text\")\n",
    "\n",
    "    with open(wav_scp, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_wav_scp = os.path.join(es_subdialect_output, \"wav.scp\")\n",
    "            for line in open(es_subdialect_wav_scp):\n",
    "                f.write(line)\n",
    "\n",
    "    with open(text_path, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_text = os.path.join(es_subdialect_output, \"text\")\n",
    "            for line in open(es_subdialect_text):\n",
    "                f.write(line)\n",
    "\n",
    "    print (output.split(\"/\")[-1], \"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  881808 check True\n",
      "dev size:  19086 check True\n",
      "test size:  19668 check True\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Mandarin', 'Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "\n",
    "all_subdialect_dir = os.path.join(output_dir, \"WD\")\n",
    "os.makedirs(all_subdialect_dir, exist_ok=True)\n",
    "all_subdialect_train = os.path.join(all_subdialect_dir, \"train\")\n",
    "WholeDevSet = os.path.join(all_subdialect_dir, \"dev\")\n",
    "WholeTestSet = os.path.join(all_subdialect_dir, \"test\")\n",
    "whole_dataset_output = [all_subdialect_train, WholeDevSet, WholeTestSet]\n",
    "\n",
    "for output in whole_dataset_output:\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    wav_scp = os.path.join(output, \"wav.scp\")\n",
    "    text_path = os.path.join(output, \"text\")\n",
    "\n",
    "    with open(wav_scp, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_wav_scp = os.path.join(es_subdialect_output, \"wav.scp\")\n",
    "            for line in open(es_subdialect_wav_scp):\n",
    "                f.write(line)\n",
    "\n",
    "    with open(text_path, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_text = os.path.join(es_subdialect_output, \"text\")\n",
    "            for line in open(es_subdialect_text):\n",
    "                f.write(line)\n",
    "\n",
    "    print (output.split(\"/\")[-1], \"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After CNAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(read_file, writen_file):\n",
    "    # 打开第一个文件以读取数据\n",
    "    with open(read_file, 'r', encoding='utf-8') as f1:\n",
    "        data = f1.read()  # 读取全部内容\n",
    "\n",
    "    # 打开第二个文件以追加数据\n",
    "    with open(writen_file, 'a', encoding='utf-8') as f2:\n",
    "        f2.write(data)  # 将读取的数据追加到文件末尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing train dataset size:  2237\n",
      "Southwestern train dataset size:  45359\n",
      "Jiao-Liao train dataset size:  20268\n",
      "Northeastern train dataset size:  4843\n",
      "Jiang-Huai train dataset size:  27586\n",
      "Lan-Yin train dataset size:  20549\n",
      "Ji-Lu train dataset size:  33861\n",
      "Zhongyuan train dataset size:  48590\n",
      "train dataset size:  203293\n",
      "Beijing dev dataset size:  296\n",
      "Southwestern dev dataset size:  2816\n",
      "Jiao-Liao dev dataset size:  1561\n",
      "Northeastern dev dataset size:  355\n",
      "Jiang-Huai dev dataset size:  2373\n",
      "Lan-Yin dev dataset size:  1750\n",
      "Ji-Lu dev dataset size:  2962\n",
      "Zhongyuan dev dataset size:  3292\n",
      "dev dataset size:  15405\n",
      "Beijing test dataset size:  265\n",
      "Southwestern test dataset size:  2684\n",
      "Jiao-Liao test dataset size:  1443\n",
      "Northeastern test dataset size:  350\n",
      "Jiang-Huai test dataset size:  2268\n",
      "Lan-Yin test dataset size:  1646\n",
      "Ji-Lu test dataset size:  2806\n",
      "Zhongyuan test dataset size:  3225\n",
      "test dataset size:  14687\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "section_list = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "all_subdialect_dir = os.path.join(output_dir, \"AS\")\n",
    "each_subdialect_dir = os.path.join(output_dir, \"ES\")\n",
    "\n",
    "\n",
    "for section in section_list:\n",
    "    all_subdialect_section = os.path.join(all_subdialect_dir, section)\n",
    "    all_subdialect_section_audio_datasets = os.path.join(all_subdialect_section, \"audio_datasets.jsonl\")\n",
    "    os.makedirs(all_subdialect_section, exist_ok=True)\n",
    "\n",
    "    for dialect in subdialect_list:\n",
    "        es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "        es_subdialect_section = os.path.join(es_subdialect_dir, section)\n",
    "        es_audio_datasets  = os.path.join(es_subdialect_section, \"audio_datasets.jsonl\")\n",
    "        print (dialect, section, \"dataset size: \", len(open(es_audio_datasets).readlines()))\n",
    "        merge_files(es_audio_datasets, all_subdialect_section_audio_datasets)\n",
    "\n",
    "    print (section, \"dataset size: \", len(open(all_subdialect_section_audio_datasets).readlines()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin train dataset size:  678515\n",
      "Beijing train dataset size:  2237\n",
      "Southwestern train dataset size:  45359\n",
      "Jiao-Liao train dataset size:  20268\n",
      "Northeastern train dataset size:  4843\n",
      "Jiang-Huai train dataset size:  27586\n",
      "Lan-Yin train dataset size:  20549\n",
      "Ji-Lu train dataset size:  33861\n",
      "Zhongyuan train dataset size:  48590\n",
      "train dataset size:  881808\n",
      "Mandarin dev dataset size:  3681\n",
      "Beijing dev dataset size:  296\n",
      "Southwestern dev dataset size:  2816\n",
      "Jiao-Liao dev dataset size:  1561\n",
      "Northeastern dev dataset size:  355\n",
      "Jiang-Huai dev dataset size:  2373\n",
      "Lan-Yin dev dataset size:  1750\n",
      "Ji-Lu dev dataset size:  2962\n",
      "Zhongyuan dev dataset size:  3292\n",
      "dev dataset size:  19086\n",
      "Mandarin test dataset size:  4981\n",
      "Beijing test dataset size:  265\n",
      "Southwestern test dataset size:  2684\n",
      "Jiao-Liao test dataset size:  1443\n",
      "Northeastern test dataset size:  350\n",
      "Jiang-Huai test dataset size:  2268\n",
      "Lan-Yin test dataset size:  1646\n",
      "Ji-Lu test dataset size:  2806\n",
      "Zhongyuan test dataset size:  3225\n",
      "test dataset size:  19668\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Mandarin', 'Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "section_list = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "whole_dir = os.path.join(output_dir, \"WD\")\n",
    "each_subdialect_dir = os.path.join(output_dir, \"ES\")\n",
    "\n",
    "for section in section_list:\n",
    "    whole_section = os.path.join(whole_dir, section)\n",
    "    whole_audio_datasets = os.path.join(whole_section, \"audio_datasets.jsonl\")\n",
    "    os.makedirs(whole_section, exist_ok=True)\n",
    "\n",
    "    for dialect in subdialect_list:\n",
    "        es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "        es_subdialect_section = os.path.join(es_subdialect_dir, section)\n",
    "        es_audio_datasets  = os.path.join(es_subdialect_section, \"audio_datasets.jsonl\")\n",
    "        print (dialect, section, \"dataset size: \", len(open(es_audio_datasets).readlines()))\n",
    "        merge_files(es_audio_datasets, whole_audio_datasets)\n",
    "\n",
    "    print (section, \"dataset size: \", len(open(whole_audio_datasets).readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 添加口音信息"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.162177Z",
     "start_time": "2024-07-29T02:47:39.185234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 源文件夹路径\n",
    "source_dir = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data\"\n",
    "# 目标文件夹路径\n",
    "data_root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2\"\n",
    "\n",
    "# 确保目标目录存在，如果不存在则创建\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "# 使用shutil.copytree来复制目录。从 Python 3.8 开始，copytree() 可以接受 dirs_exist_ok 参数\n",
    "# 如果目标目录已存在，并且你想要覆盖里面的文件，则需要设置 dirs_exist_ok=True\n",
    "try:\n",
    "    shutil.copytree(source_dir, data_root, dirs_exist_ok=True)\n",
    "except FileExistsError:\n",
    "    # 如果在较早的 Python 版本中使用 shutil.copytree 且目标目录已存在，会引发此错误\n",
    "    print(\"目录已存在\")\n",
    "except Exception as e:\n",
    "    print(f\"复制过程中出错: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.172035Z",
     "start_time": "2024-07-29T02:47:40.164904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "end_name = \"audio_datasets.jsonl\"\n",
    "\n",
    "# 遍历所有的.jsonl文件\n",
    "data_list = []\n",
    "for root, dirs, files in os.walk(data_root):\n",
    "    for file in files:\n",
    "        if file == end_name:\n",
    "            file_path = os.path.join(root, file)\n",
    "            data_list.append(file_path)\n",
    "            \n",
    "print (data_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/train/audio_datasets.jsonl']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.482450Z",
     "start_time": "2024-07-29T02:47:40.173550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取口音文件并转换成字典\n",
    "dialect_root = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Tasks/SubdialectID\"\n",
    "dialect_dict = {}\n",
    "for root, dirs, files in os.walk(dialect_root):\n",
    "    for file in files:\n",
    "        if file == \"utt2subdialect\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    id, dialect = line.split(\" \")\n",
    "                    dialect_dict[id] = dialect"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.489388Z",
     "start_time": "2024-07-29T02:47:40.484407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_dialect(lines, dialect_dict, path):\n",
    "    \n",
    "    save_path = path.replace(\"audio_datasets\", \"audio_datasets2\")\n",
    "\n",
    "    import json\n",
    "    \n",
    "    updated_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # 解析 JSON 数据\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # 根据 key 在 dialect_dict 中查找对应的 dialect\n",
    "        key = data['key']\n",
    "        dialect = dialect_dict.get(key, \"Mandarin\")  # 如果 key 不存在于 dialect_dict 中，则默认值为 \"Unknown\"\n",
    "        \n",
    "        # 添加 \"dialect\" 字段\n",
    "        data['text_language'] = dialect\n",
    "        \n",
    "        # 将更新后的 JSON 数据转回字符串\n",
    "        updated_line = json.dumps(data, ensure_ascii=False) + '\\n'\n",
    "        updated_lines.append(updated_line)\n",
    "    \n",
    "    # 将更新后的内容写回文件\n",
    "    with open(save_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(updated_lines)\n",
    "        \n",
    "    os.remove(path)\n",
    "    os.renames(save_path, path)\n",
    "    \n",
    "    print(\"文件更新完成。\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.568448Z",
     "start_time": "2024-07-29T02:47:40.490438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_10_items = list(dialect_dict.items())[:10]\n",
    "\n",
    "# 打印前10个项目\n",
    "for key, value in first_10_items:\n",
    "    print(f'{key}: {value}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000001_0b1a33a3: Mandarin\n",
      "1000001_0e9793ff: Mandarin\n",
      "1000001_11f3978b: Mandarin\n",
      "1000001_1c4b6ce5: Mandarin\n",
      "1000001_2c863844: Mandarin\n",
      "1000001_3c84b37d: Mandarin\n",
      "1000001_492740a5: Mandarin\n",
      "1000001_5c8b5985: Mandarin\n",
      "1000001_63740949: Mandarin\n",
      "1000001_6a7435f1: Mandarin\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.429008Z",
     "start_time": "2024-07-29T02:47:40.569576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(data_list)):\n",
    "# for i in range (1):\n",
    "    data_path = data_list[i]\n",
    "    print (\"Processing: \", data_path)\n",
    "    with open(data_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    add_dialect(lines, dialect_dict, data_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/train/audio_datasets.jsonl\n",
      "文件更新完成。\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 新建口音字典"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.434507Z",
     "start_time": "2024-07-29T02:48:05.430618Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.665939Z",
     "start_time": "2024-07-29T02:48:05.435822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取口音文件并转换成字典\n",
    "dialect_root = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Tasks/SubdialectID\"\n",
    "dialect_list = []\n",
    "for root, dirs, files in os.walk(dialect_root):\n",
    "    for file in files:\n",
    "        if file == \"utt2subdialect\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    id, dialect = line.split(\" \")\n",
    "                    if dialect not in dialect_list:\n",
    "                        dialect_list.append(dialect)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.670980Z",
     "start_time": "2024-07-29T02:48:05.667290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_dialect_path = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/zh_token_list/char/dialects.txt\"\n",
    "with open(save_dialect_path, 'w') as f:\n",
    "    for dialect in dialect_list:\n",
    "        f.write(f\"{dialect}\\n\")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.675516Z",
     "start_time": "2024-07-29T02:48:05.673288Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 将重复的内容清洗掉"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T03:55:05.608895Z",
     "start_time": "2024-07-31T03:55:05.598210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T03:56:09.422581Z",
     "start_time": "2024-07-31T03:56:09.410231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pending_list = []\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            pending_list.append(file_path)\n",
    "print (pending_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.bac.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.bac.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.jsonl']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T03:57:12.462089Z",
     "start_time": "2024-07-31T03:57:12.450986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    print (file_path, \"size: \", len(lines))\n",
    "    lines = list(set(lines))\n",
    "    print (file_path, \"size: \", len(lines))\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.writelines(lines)\n",
    "    print (file_path, \"cleaned!\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T03:57:19.899621Z",
     "start_time": "2024-07-31T03:57:16.627782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file_path in pending_list:\n",
    "    clean_file(file_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets.jsonl size:  2268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets.jsonl size:  2268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets.jsonl size:  2373\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets.jsonl size:  2373\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets.jsonl size:  27586\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets.jsonl size:  27586\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets.jsonl size:  1646\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets.jsonl size:  1646\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets.jsonl size:  1750\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets.jsonl size:  1750\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets.jsonl size:  20549\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets.jsonl size:  20549\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets.jsonl size:  350\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets.jsonl size:  350\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets.jsonl size:  355\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets.jsonl size:  355\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets.jsonl size:  4843\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets.jsonl size:  4843\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets.jsonl size:  2684\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets.jsonl size:  2684\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets.jsonl size:  2816\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets.jsonl size:  2816\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets.jsonl size:  45359\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets.jsonl size:  45359\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets.jsonl size:  2806\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets.jsonl size:  2806\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets.jsonl size:  2962\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets.jsonl size:  2962\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.bac.jsonl size:  33861\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.bac.jsonl size:  33861\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.bac.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.jsonl size:  101583\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.jsonl size:  33861\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets.jsonl size:  3225\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets.jsonl size:  3225\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets.jsonl size:  3292\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets.jsonl size:  3292\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets.jsonl size:  48590\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets.jsonl size:  48590\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets.jsonl size:  265\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets.jsonl size:  265\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets.jsonl size:  296\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets.jsonl size:  296\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets.jsonl size:  2237\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets.jsonl size:  2237\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets.jsonl size:  4981\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets.jsonl size:  4981\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets.jsonl size:  3681\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets.jsonl size:  3681\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets.jsonl size:  678515\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets.jsonl size:  678515\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets.jsonl size:  1443\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets.jsonl size:  1443\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets.jsonl size:  1561\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets.jsonl size:  1561\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.bac.jsonl size:  20268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.bac.jsonl size:  20268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.bac.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.jsonl size:  101340\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.jsonl size:  20268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.jsonl cleaned!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T04:04:57.736023Z",
     "start_time": "2024-07-31T04:04:45.750347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/\"\n",
    "\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if \"bac\" in file:\n",
    "            print (file)\n",
    "            os.remove(os.path.join(root, file))\n",
    "            print (file, \"removed!\")\n",
    "        if \"hubert\" in file:\n",
    "            print (file)\n",
    "            os.remove(os.path.join(root, file))\n",
    "            print (file, \"removed!\")\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "audio_datasets.bac.jsonl\n",
      "audio_datasets.bac.jsonl removed!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 将data3里面的所有phase2都去掉"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T10:14:05.647662Z",
     "start_time": "2024-08-04T10:14:05.643324Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T10:14:07.065826Z",
     "start_time": "2024-08-04T10:14:06.637121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3\"\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            if not file_path.endswith(\"audio_datasets.jsonl\"):\n",
    "                print (file_path)\n",
    "                os.remove(file_path)\n",
    "            # print (file_path)\n",
    "            # with open(file_path, 'r') as f:\n",
    "            #     lines = f.readlines()\n",
    "            # new_lines = []\n",
    "            # for line in lines:\n",
    "            #     if \"phase2\" not in line:\n",
    "            #         new_lines.append(line)\n",
    "            # clean_file_path = file_path.replace(\"audio_datasets\", \"audio_datasets_phase1\")\n",
    "            # with open(clean_file_path, 'w') as f:\n",
    "            #     f.writelines(new_lines)\n",
    "            "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/AS/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/AS/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/AS/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/AS/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/AS/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/AS/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/WD/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/WD/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/WD/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/WD/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/WD/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/WD/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/MD/test/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/MD/test/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/MD/dev/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/MD/dev/audio_datasets_phase1_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/MD/train/audio_datasets_phase1.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/MD/train/audio_datasets_phase1_phase1.jsonl\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T10:14:25.062541Z",
     "start_time": "2024-08-04T10:14:09.744574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3\"\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\"audio_datasets.jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print (file_path)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            new_lines = []\n",
    "            for line in lines:\n",
    "                if \"phase2\" not in line:\n",
    "                    new_lines.append(line)\n",
    "            clean_file_path = file_path.replace(\"audio_datasets\", \"audio_datasets_phase1\")\n",
    "            with open(clean_file_path, 'w') as f:\n",
    "                f.writelines(new_lines)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/AS/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/AS/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/AS/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/WD/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/WD/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/WD/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/MD/test/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/MD/dev/audio_datasets.jsonl\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/MD/train/audio_datasets.jsonl\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T10:37:10.821399Z",
     "start_time": "2024-08-04T10:37:05.284862Z"
    }
   },
   "cell_type": "code",
   "source": "!nvidia-smi",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug  4 18:37:09 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:31:00.0 Off |                  Off |\r\n",
      "|  0%   37C    P0             44W /  450W |       1MiB /  24564MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:4B:00.0 Off |                  Off |\r\n",
      "|  0%   36C    P0             59W /  450W |       1MiB /  24564MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   2  NVIDIA GeForce RTX 4090        Off |   00000000:B1:00.0 Off |                  Off |\r\n",
      "|  0%   37C    P0             58W /  450W |       1MiB /  24564MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   3  NVIDIA GeForce RTX 4090        Off |   00000000:CA:00.0 Off |                  Off |\r\n",
      "|  0%   37C    P0             59W /  450W |       1MiB /  24564MiB |      1%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 统计语音数量"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T07:32:57.261523Z",
     "start_time": "2024-08-05T07:32:57.252666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES\"\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\"audio_datasets_phase1.jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_list.append(file_path)\n",
    "print(file_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets_phase1.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets_phase1.jsonl']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T07:34:38.476051Z",
     "start_time": "2024-08-05T07:34:36.539171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 统计各个文件里的语音数量\n",
    "len_dict = {}\n",
    "for file_path in file_list:\n",
    "    if \"test\" in file_path:\n",
    "        continue\n",
    "    id = file_path.split(\"/\")[-3]\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if id not in len_dict:\n",
    "        len_dict[id] = len(lines)\n",
    "    else:\n",
    "        len_dict[id] += len(lines)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T07:34:39.847601Z",
     "start_time": "2024-08-05T07:34:39.838413Z"
    }
   },
   "cell_type": "code",
   "source": "len_dict",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Northeastern': 5198,\n",
       " 'Jiao-Liao': 21829,\n",
       " 'Southwestern': 48175,\n",
       " 'Jiang-Huai': 29959,\n",
       " 'Beijing': 2533,\n",
       " 'Mandarin': 340882,\n",
       " 'Ji-Lu': 36823,\n",
       " 'Zhongyuan': 51882,\n",
       " 'Lan-Yin': 22299}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T07:38:00.481221Z",
     "start_time": "2024-08-05T07:38:00.473781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "report_len = {\"Mandarin\": 370819, \"Beijing\": 2538, \"Southwestern\": 48465, \"Jiao-Liao\": 21847, \"Northeastern\": 5205, \"Jiang-Huai\": 30008, \"Lan-Yin\":\n",
    "    22324, \"Ji-Lu\": 36921, \"Zhongyuan\": 52012}"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T07:38:22.930536Z",
     "start_time": "2024-08-05T07:38:22.922736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 差值\n",
    "diff_dict = {}\n",
    "for key in len_dict:\n",
    "    diff_dict[key] = report_len[key] - len_dict[key]\n",
    "print (diff_dict)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Northeastern': 7, 'Jiao-Liao': 18, 'Southwestern': 290, 'Jiang-Huai': 49, 'Beijing': 5, 'Mandarin': 29937, 'Ji-Lu': 98, 'Zhongyuan': 130, 'Lan-Yin': 25}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T07:42:40.799902Z",
     "start_time": "2024-08-05T07:42:40.776866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "utt2dia = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Tasks/ASR/test/utt2subdialect\"\n",
    "dia_dict = {}\n",
    "with open(utt2dia, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, dia = line.split(\" \")\n",
    "        if \"p2\" in id:\n",
    "            continue\n",
    "        if dia not in dia_dict:\n",
    "            dia_dict[dia] = 1\n",
    "        else:\n",
    "            dia_dict[dia] += 1"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T07:42:47.369993Z",
     "start_time": "2024-08-05T07:42:47.360701Z"
    }
   },
   "cell_type": "code",
   "source": "dia_dict",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Northeastern': 351,\n",
       " 'Mandarin': 2045,\n",
       " 'Ji-Lu': 2809,\n",
       " 'Zhongyuan': 3239,\n",
       " 'Jiao-Liao': 1443,\n",
       " 'Jiang-Huai': 2271,\n",
       " 'Beijing': 265,\n",
       " 'Southwestern': 2693,\n",
       " 'Lan-Yin': 1652}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T07:43:22.735682Z",
     "start_time": "2024-08-05T07:43:22.665209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "len_dict = {}\n",
    "for file_path in file_list:\n",
    "    if \"test\" not in file_path:\n",
    "        continue\n",
    "    id = file_path.split(\"/\")[-3]\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if id not in len_dict:\n",
    "        len_dict[id] = len(lines)\n",
    "    else:\n",
    "        len_dict[id] += len(lines)\n",
    "len_dict"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Northeastern': 350,\n",
       " 'Jiao-Liao': 1443,\n",
       " 'Southwestern': 2684,\n",
       " 'Jiang-Huai': 2268,\n",
       " 'Beijing': 265,\n",
       " 'Mandarin': 2040,\n",
       " 'Ji-Lu': 2806,\n",
       " 'Zhongyuan': 3225,\n",
       " 'Lan-Yin': 1646}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T07:44:12.837193Z",
     "start_time": "2024-08-05T07:44:12.829312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "diff_dict = {}\n",
    "for key in len_dict:\n",
    "    diff_dict[key] = dia_dict[key] - len_dict[key]\n",
    "diff_dict"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Northeastern': 1,\n",
       " 'Jiao-Liao': 0,\n",
       " 'Southwestern': 9,\n",
       " 'Jiang-Huai': 3,\n",
       " 'Beijing': 0,\n",
       " 'Mandarin': 5,\n",
       " 'Ji-Lu': 3,\n",
       " 'Zhongyuan': 14,\n",
       " 'Lan-Yin': 6}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:07:31.218593Z",
     "start_time": "2024-08-05T08:07:29.633990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "dia_dict = {}\n",
    "contrast_dev_path = \"/data/NAS_PLUS/zhuang/dataset/data_KeSpeech/KeSpeech/Tasks/ASContrastive/train_data.list\"\n",
    "with open(contrast_dev_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    data = json.loads(line)\n",
    "    sample_acc = data[\"sample_acc\"]\n",
    "    if sample_acc not in dia_dict:\n",
    "        dia_dict[sample_acc] = 1\n",
    "    else:\n",
    "        dia_dict[sample_acc] += 1"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:07:33.205962Z",
     "start_time": "2024-08-05T08:07:33.196767Z"
    }
   },
   "cell_type": "code",
   "source": "dia_dict",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Zhongyuan': 14753,\n",
       " 'Northeastern': 1766,\n",
       " 'Ji-Lu': 13000,\n",
       " 'Mandarin': 136415,\n",
       " 'Southwestern': 14117,\n",
       " 'Jiao-Liao': 7169,\n",
       " 'Beijing': 784,\n",
       " 'Jiang-Huai': 9623,\n",
       " 'Lan-Yin': 6716}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 更新data2到data4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:59:25.946182Z",
     "start_time": "2024-08-15T15:59:25.932589Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:59:32.276814Z",
     "start_time": "2024-08-15T15:59:29.859475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_2 = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/\"\n",
    "data_4 = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/\"\n",
    "if not os.path.exists(data_4):\n",
    "    os.makedirs(data_4)\n",
    "\n",
    "# copy data2 to data4\n",
    "for root, dirs, files in os.walk(data_2):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        new_file_path = file_path.replace(\"data2\", \"data4\")\n",
    "        os.makedirs(os.path.dirname(new_file_path), exist_ok=True)\n",
    "        os.system(f\"cp {file_path} {new_file_path}\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:07:33.926932Z",
     "start_time": "2024-08-15T16:07:25.668090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data4中WD里的所有audio_datasets.jsonl中\"source\"不含phase1对应的text_language设置成None\n",
    "import json\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/WD\"\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\"audio_datasets.jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print (file_path)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            new_lines = []\n",
    "            for line in lines:\n",
    "                item = json.loads(line)\n",
    "                if \"phase1\" not in item[\"source\"]:\n",
    "                    item[\"text_language\"] = None\n",
    "                    \n",
    "                \n",
    "                new_lines.append(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            print (new_lines[:10])\n",
    "            clean_file_path = file_path.replace(\"audio_datasets\", \"audio_datasets_2\")\n",
    "            with open(clean_file_path, 'w') as f:\n",
    "                f.writelines(new_lines)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/WD/test/audio_datasets.jsonl\n",
      "['{\"key\": \"1005596_p2_b29a8b0d\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005596/phase2/1005596_p2_b29a8b0d.wav\", \"source_len\": 701, \"target\": \"四 招 消 除 隐 私 外 泄 风 险 注 销\", \"target_len\": 12, \"text_language\": null}\\n', '{\"key\": \"1005598_p2_65747a0c\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005598/phase2/1005598_p2_65747a0c.wav\", \"source_len\": 558, \"target\": \"直 到 你 绝 望 之 后 参 加 他 们 的 旅 行 为 止\", \"target_len\": 16, \"text_language\": null}\\n', '{\"key\": \"1005608_42545eba\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005608/phase1/1005608_42545eba.wav\", \"source_len\": 469, \"target\": \"一 百 零 八 元 极 食 生 活 美 学 即 艺 术 美 食\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005608_p2_29a38a6f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005608/phase2/1005608_p2_29a38a6f.wav\", \"source_len\": 263, \"target\": \"十 分 钟 后 王 女 士 取 消 订 单\", \"target_len\": 11, \"text_language\": null}\\n', '{\"key\": \"1005610_4959cdde\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase1/1005610_4959cdde.wav\", \"source_len\": 731, \"target\": \"是 一 九 九 二 年 四 月 一 日 开 始 实 施 的\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005610_f637f4ff\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase1/1005610_f637f4ff.wav\", \"source_len\": 645, \"target\": \"创 造 出 独 一 无 二 的 地 道 澳 门 葡 国 菜\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005610_p2_6b6b1b1a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase2/1005610_p2_6b6b1b1a.wav\", \"source_len\": 533, \"target\": \"是 一 九 九 二 年 四 月 一 日 开 始 实 施 的\", \"target_len\": 15, \"text_language\": null}\\n', '{\"key\": \"1005610_p2_b0fc4e89\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase2/1005610_p2_b0fc4e89.wav\", \"source_len\": 485, \"target\": \"并 确 保 在 蛋 糕 保 鲜 期 内 食 用\", \"target_len\": 12, \"text_language\": null}\\n', '{\"key\": \"1005621_69034f9e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005621/phase1/1005621_69034f9e.wav\", \"source_len\": 767, \"target\": \"吃 蟹 时 和 吃 蟹 后 一 小 时 内 忌 饮 茶 水\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005621_p2_57cc9398\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005621/phase2/1005621_p2_57cc9398.wav\", \"source_len\": 421, \"target\": \"可 能 意 图 在 于 想 要 提 醒 双 方\", \"target_len\": 12, \"text_language\": null}\\n']\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/WD/dev/audio_datasets.jsonl\n",
      "['{\"key\": \"1029619_p2_ba7b287e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_ba7b287e.wav\", \"source_len\": 419, \"target\": \"尼 克 斯 靠 安 东 尼 和 罗 斯 稳 定 局 面\", \"target_len\": 14, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_c10e80ac\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_c10e80ac.wav\", \"source_len\": 319, \"target\": \"此 需 要 不 断 的 复 习 来 强 化 记 忆\", \"target_len\": 13, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_d15ea33f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_d15ea33f.wav\", \"source_len\": 991, \"target\": \"借 亲 人 的 名 义 陆 续 在 广 州 购 置 四 套 物 业\", \"target_len\": 17, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_d1c2c68b\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_d1c2c68b.wav\", \"source_len\": 453, \"target\": \"电 控 燃 油 缸 内 直 喷 涡 轮 发 动 机\", \"target_len\": 13, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_d34d4c2a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_d34d4c2a.wav\", \"source_len\": 991, \"target\": \"做 你 自 己 的 分 析 不 管 其 它 的 信 息 来 源\", \"target_len\": 16, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_de5e7dd7\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_de5e7dd7.wav\", \"source_len\": 379, \"target\": \"这 些 无 人 机 被 挂 上 了 带 有 广 告 的 硬 纸 板\", \"target_len\": 17, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_f111e451\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_f111e451.wav\", \"source_len\": 497, \"target\": \"可 能 会 需 要 更 多 的 时 间 来 发 展 人 工 智 能\", \"target_len\": 17, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_f2cb1f03\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_f2cb1f03.wav\", \"source_len\": 467, \"target\": \"连 续 两 个 或 两 个 以 上 开 放 日 发 生 巨 额 赎 回\", \"target_len\": 18, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_fd584b1a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_fd584b1a.wav\", \"source_len\": 465, \"target\": \"多 次 登 上 全 国 各 大 卫 视 进 行 表 演\", \"target_len\": 14, \"text_language\": null}\\n', '{\"key\": \"1029722_p2_02f6c59c\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029722/phase2/1029722_p2_02f6c59c.wav\", \"source_len\": 487, \"target\": \"历 经 生 死 攸 关 的 独 树 镇 血 战\", \"target_len\": 12, \"text_language\": null}\\n']\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/WD/train/audio_datasets.jsonl\n",
      "['{\"key\": \"1007380_706904a1\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_706904a1.wav\", \"source_len\": 458, \"target\": \"巡 展 组 委 会 通 过 车 展 中 国 的 平 台\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_8bc9430d\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_8bc9430d.wav\", \"source_len\": 825, \"target\": \"居 民 聚 集 地 等 环 境 敏 感 点 为 基 础\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_8cae8266\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_8cae8266.wav\", \"source_len\": 366, \"target\": \"工 作 九 年 的 人 资\", \"target_len\": 7, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_993adf51\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_993adf51.wav\", \"source_len\": 987, \"target\": \"哎 你 们 一 定 要 多 努 力 多 支 持 他 的 工 作\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9c2c9d1f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9c2c9d1f.wav\", \"source_len\": 426, \"target\": \"工 作 人 员 正 在 对 供 暖 设 备 进 行 检 修\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9f3dee01\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9f3dee01.wav\", \"source_len\": 511, \"target\": \"尽 管 海 军 定 期 举 行 类 似 演 练\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9faf9fce\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9faf9fce.wav\", \"source_len\": 415, \"target\": \"工 作 要 求 推 动 各 项 工 作 落 实\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9ff2071f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9ff2071f.wav\", \"source_len\": 159, \"target\": \"你 们 来 多 少 人\", \"target_len\": 6, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_a013c66e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_a013c66e.wav\", \"source_len\": 529, \"target\": \"届 时 可 能 会 新 增 及 改 造 地 下 换 乘\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_b006108c\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_b006108c.wav\", \"source_len\": 983, \"target\": \"尽 管 意 大 利 的 联 赛 里 不 缺 高 优 秀 高 大 的 后 卫\", \"target_len\": 19, \"text_language\": \"Mandarin\"}\\n']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:09:59.372391Z",
     "start_time": "2024-08-15T16:09:58.461395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 随机读取几个文件检查\n",
    "import json\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/WD\"\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\"audio_datasets_2.jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print (file_path)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            print (lines[:10])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/WD/test/audio_datasets_2.jsonl\n",
      "['{\"key\": \"1005596_p2_b29a8b0d\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005596/phase2/1005596_p2_b29a8b0d.wav\", \"source_len\": 701, \"target\": \"四 招 消 除 隐 私 外 泄 风 险 注 销\", \"target_len\": 12, \"text_language\": null}\\n', '{\"key\": \"1005598_p2_65747a0c\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005598/phase2/1005598_p2_65747a0c.wav\", \"source_len\": 558, \"target\": \"直 到 你 绝 望 之 后 参 加 他 们 的 旅 行 为 止\", \"target_len\": 16, \"text_language\": null}\\n', '{\"key\": \"1005608_42545eba\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005608/phase1/1005608_42545eba.wav\", \"source_len\": 469, \"target\": \"一 百 零 八 元 极 食 生 活 美 学 即 艺 术 美 食\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005608_p2_29a38a6f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005608/phase2/1005608_p2_29a38a6f.wav\", \"source_len\": 263, \"target\": \"十 分 钟 后 王 女 士 取 消 订 单\", \"target_len\": 11, \"text_language\": null}\\n', '{\"key\": \"1005610_4959cdde\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase1/1005610_4959cdde.wav\", \"source_len\": 731, \"target\": \"是 一 九 九 二 年 四 月 一 日 开 始 实 施 的\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005610_f637f4ff\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase1/1005610_f637f4ff.wav\", \"source_len\": 645, \"target\": \"创 造 出 独 一 无 二 的 地 道 澳 门 葡 国 菜\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005610_p2_6b6b1b1a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase2/1005610_p2_6b6b1b1a.wav\", \"source_len\": 533, \"target\": \"是 一 九 九 二 年 四 月 一 日 开 始 实 施 的\", \"target_len\": 15, \"text_language\": null}\\n', '{\"key\": \"1005610_p2_b0fc4e89\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase2/1005610_p2_b0fc4e89.wav\", \"source_len\": 485, \"target\": \"并 确 保 在 蛋 糕 保 鲜 期 内 食 用\", \"target_len\": 12, \"text_language\": null}\\n', '{\"key\": \"1005621_69034f9e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005621/phase1/1005621_69034f9e.wav\", \"source_len\": 767, \"target\": \"吃 蟹 时 和 吃 蟹 后 一 小 时 内 忌 饮 茶 水\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005621_p2_57cc9398\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005621/phase2/1005621_p2_57cc9398.wav\", \"source_len\": 421, \"target\": \"可 能 意 图 在 于 想 要 提 醒 双 方\", \"target_len\": 12, \"text_language\": null}\\n']\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/WD/dev/audio_datasets_2.jsonl\n",
      "['{\"key\": \"1029619_p2_ba7b287e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_ba7b287e.wav\", \"source_len\": 419, \"target\": \"尼 克 斯 靠 安 东 尼 和 罗 斯 稳 定 局 面\", \"target_len\": 14, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_c10e80ac\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_c10e80ac.wav\", \"source_len\": 319, \"target\": \"此 需 要 不 断 的 复 习 来 强 化 记 忆\", \"target_len\": 13, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_d15ea33f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_d15ea33f.wav\", \"source_len\": 991, \"target\": \"借 亲 人 的 名 义 陆 续 在 广 州 购 置 四 套 物 业\", \"target_len\": 17, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_d1c2c68b\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_d1c2c68b.wav\", \"source_len\": 453, \"target\": \"电 控 燃 油 缸 内 直 喷 涡 轮 发 动 机\", \"target_len\": 13, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_d34d4c2a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_d34d4c2a.wav\", \"source_len\": 991, \"target\": \"做 你 自 己 的 分 析 不 管 其 它 的 信 息 来 源\", \"target_len\": 16, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_de5e7dd7\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_de5e7dd7.wav\", \"source_len\": 379, \"target\": \"这 些 无 人 机 被 挂 上 了 带 有 广 告 的 硬 纸 板\", \"target_len\": 17, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_f111e451\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_f111e451.wav\", \"source_len\": 497, \"target\": \"可 能 会 需 要 更 多 的 时 间 来 发 展 人 工 智 能\", \"target_len\": 17, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_f2cb1f03\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_f2cb1f03.wav\", \"source_len\": 467, \"target\": \"连 续 两 个 或 两 个 以 上 开 放 日 发 生 巨 额 赎 回\", \"target_len\": 18, \"text_language\": null}\\n', '{\"key\": \"1029619_p2_fd584b1a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029619/phase2/1029619_p2_fd584b1a.wav\", \"source_len\": 465, \"target\": \"多 次 登 上 全 国 各 大 卫 视 进 行 表 演\", \"target_len\": 14, \"text_language\": null}\\n', '{\"key\": \"1029722_p2_02f6c59c\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1029722/phase2/1029722_p2_02f6c59c.wav\", \"source_len\": 487, \"target\": \"历 经 生 死 攸 关 的 独 树 镇 血 战\", \"target_len\": 12, \"text_language\": null}\\n']\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/WD/train/audio_datasets_2.jsonl\n",
      "['{\"key\": \"1007380_706904a1\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_706904a1.wav\", \"source_len\": 458, \"target\": \"巡 展 组 委 会 通 过 车 展 中 国 的 平 台\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_8bc9430d\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_8bc9430d.wav\", \"source_len\": 825, \"target\": \"居 民 聚 集 地 等 环 境 敏 感 点 为 基 础\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_8cae8266\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_8cae8266.wav\", \"source_len\": 366, \"target\": \"工 作 九 年 的 人 资\", \"target_len\": 7, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_993adf51\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_993adf51.wav\", \"source_len\": 987, \"target\": \"哎 你 们 一 定 要 多 努 力 多 支 持 他 的 工 作\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9c2c9d1f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9c2c9d1f.wav\", \"source_len\": 426, \"target\": \"工 作 人 员 正 在 对 供 暖 设 备 进 行 检 修\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9f3dee01\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9f3dee01.wav\", \"source_len\": 511, \"target\": \"尽 管 海 军 定 期 举 行 类 似 演 练\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9faf9fce\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9faf9fce.wav\", \"source_len\": 415, \"target\": \"工 作 要 求 推 动 各 项 工 作 落 实\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9ff2071f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9ff2071f.wav\", \"source_len\": 159, \"target\": \"你 们 来 多 少 人\", \"target_len\": 6, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_a013c66e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_a013c66e.wav\", \"source_len\": 529, \"target\": \"届 时 可 能 会 新 增 及 改 造 地 下 换 乘\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_b006108c\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_b006108c.wav\", \"source_len\": 983, \"target\": \"尽 管 意 大 利 的 联 赛 里 不 缺 高 优 秀 高 大 的 后 卫\", \"target_len\": 19, \"text_language\": \"Mandarin\"}\\n']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:12:08.025792Z",
     "start_time": "2024-08-15T16:12:07.972671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把data4中WD里的所有audio_datasets_2.jsonl改成audio_datasets.jsonl\n",
    "import os\n",
    "\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/WD\"\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\"audio_datasets_2.jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            new_file_path = file_path.replace(\"audio_datasets_2\", \"audio_datasets\")\n",
    "            os.rename(file_path, new_file_path)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 更新data4到data5\n",
    "data5只有phase1的内容"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T01:42:18.576512Z",
     "start_time": "2024-08-27T01:42:18.566503Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T01:43:39.851131Z",
     "start_time": "2024-08-27T01:43:38.334586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_4 = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data4/\"\n",
    "data_5 = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/\"\n",
    "if not os.path.exists(data_5):\n",
    "    os.makedirs(data_5)\n",
    "\n",
    "# copy data2 to data4\n",
    "for root, dirs, files in os.walk(data_4):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        new_file_path = file_path.replace(\"data4\", \"data5\")\n",
    "        os.makedirs(os.path.dirname(new_file_path), exist_ok=True)\n",
    "        os.system(f\"cp {file_path} {new_file_path}\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T01:55:01.278230Z",
     "start_time": "2024-08-27T01:54:53.390078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data4中WD里的所有audio_datasets.jsonl中\"source\"不含phase1对应的text_language设置成None\n",
    "import json\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD\"\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\"audio_datasets.jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print (file_path)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            new_lines = []\n",
    "            for line in lines:\n",
    "                item = json.loads(line)\n",
    "                if \"phase1\" not in item[\"source\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_lines.append(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            print (new_lines[:10])\n",
    "            clean_file_path = file_path.replace(\"audio_datasets\", \"audio_datasets_2\")\n",
    "            with open(clean_file_path, 'w') as f:\n",
    "                f.writelines(new_lines)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/test/audio_datasets.jsonl\n",
      "['{\"key\": \"1005608_42545eba\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005608/phase1/1005608_42545eba.wav\", \"source_len\": 469, \"target\": \"一 百 零 八 元 极 食 生 活 美 学 即 艺 术 美 食\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005610_4959cdde\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase1/1005610_4959cdde.wav\", \"source_len\": 731, \"target\": \"是 一 九 九 二 年 四 月 一 日 开 始 实 施 的\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005610_f637f4ff\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase1/1005610_f637f4ff.wav\", \"source_len\": 645, \"target\": \"创 造 出 独 一 无 二 的 地 道 澳 门 葡 国 菜\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005621_69034f9e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005621/phase1/1005621_69034f9e.wav\", \"source_len\": 767, \"target\": \"吃 蟹 时 和 吃 蟹 后 一 小 时 内 忌 饮 茶 水\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005647_4d5beaa1\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005647/phase1/1005647_4d5beaa1.wav\", \"source_len\": 645, \"target\": \"下 穿 蓝 色 小 短 裤 露 出 皮 带 显 得 时 尚 美 艳\", \"target_len\": 17, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005647_7e966afc\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005647/phase1/1005647_7e966afc.wav\", \"source_len\": 511, \"target\": \"不 久 后 亚 历 山 大 将 去 乌 克 兰 首 都 基 辅\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005647_d28e01c4\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005647/phase1/1005647_d28e01c4.wav\", \"source_len\": 551, \"target\": \"下 半 场 马 竞 整 体 压 上 的 很 凶\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005699_7860665a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005699/phase1/1005699_7860665a.wav\", \"source_len\": 589, \"target\": \"中 国 联 通 公 司 为 推 广 电 话 卡\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005716_7a2d3edb\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005716/phase1/1005716_7a2d3edb.wav\", \"source_len\": 747, \"target\": \"国 家 统 计 局 新 闻 发 言 人 盛 来 运 在 昨 天\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005742_bedf0f6a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005742/phase1/1005742_bedf0f6a.wav\", \"source_len\": 493, \"target\": \"布 林 线 提 供 的 进 出 场 点 较 为 明 确\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n']\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/dev/audio_datasets.jsonl\n",
      "['{\"key\": \"1023032_ec212044\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023032/phase1/1023032_ec212044.wav\", \"source_len\": 579, \"target\": \"在 其 他 国 家 库 存 数 据 则 不 太 完 整\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023032_f50aeab2\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023032/phase1/1023032_f50aeab2.wav\", \"source_len\": 368, \"target\": \"在 你 看 来 可 不 是 那 么 一 回 事\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_16f0e3bd\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_16f0e3bd.wav\", \"source_len\": 509, \"target\": \"你 才 能 够 去 得 到 你 想 要 的 利 润\", \"target_len\": 13, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_2ccd7938\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_2ccd7938.wav\", \"source_len\": 471, \"target\": \"散 户 相 信 中 国 政 府 会 继 续 采 取 措 施\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_4f79ecf9\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_4f79ecf9.wav\", \"source_len\": 405, \"target\": \"除 沙 特 石 油 部 长 无 法 到 场 外\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_55a4f4d3\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_55a4f4d3.wav\", \"source_len\": 347, \"target\": \"比 如 阿 莫 西 林 或 头 孢 一 类 的\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_79ecb3c9\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_79ecb3c9.wav\", \"source_len\": 605, \"target\": \"德 翼 航 空 获 赔 三 亿 用 于 坠 机 赔 偿\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_d71ffc52\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_d71ffc52.wav\", \"source_len\": 597, \"target\": \"发 送 检 验 检 疫 机 构 指 定 的 邮 箱 中\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_e7676ff9\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_e7676ff9.wav\", \"source_len\": 459, \"target\": \"新 世 纪 等 强 队 也 非 常 有 实 力\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023588_2a383d8f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023588/phase1/1023588_2a383d8f.wav\", \"source_len\": 493, \"target\": \"以 前 羞 于 和 别 人 谈 论 过 去 的 事 情\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n']\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/train/audio_datasets.jsonl\n",
      "['{\"key\": \"1007380_706904a1\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_706904a1.wav\", \"source_len\": 458, \"target\": \"巡 展 组 委 会 通 过 车 展 中 国 的 平 台\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_8bc9430d\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_8bc9430d.wav\", \"source_len\": 825, \"target\": \"居 民 聚 集 地 等 环 境 敏 感 点 为 基 础\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_8cae8266\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_8cae8266.wav\", \"source_len\": 366, \"target\": \"工 作 九 年 的 人 资\", \"target_len\": 7, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_993adf51\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_993adf51.wav\", \"source_len\": 987, \"target\": \"哎 你 们 一 定 要 多 努 力 多 支 持 他 的 工 作\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9c2c9d1f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9c2c9d1f.wav\", \"source_len\": 426, \"target\": \"工 作 人 员 正 在 对 供 暖 设 备 进 行 检 修\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9f3dee01\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9f3dee01.wav\", \"source_len\": 511, \"target\": \"尽 管 海 军 定 期 举 行 类 似 演 练\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9faf9fce\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9faf9fce.wav\", \"source_len\": 415, \"target\": \"工 作 要 求 推 动 各 项 工 作 落 实\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9ff2071f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9ff2071f.wav\", \"source_len\": 159, \"target\": \"你 们 来 多 少 人\", \"target_len\": 6, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_a013c66e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_a013c66e.wav\", \"source_len\": 529, \"target\": \"届 时 可 能 会 新 增 及 改 造 地 下 换 乘\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_b006108c\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_b006108c.wav\", \"source_len\": 983, \"target\": \"尽 管 意 大 利 的 联 赛 里 不 缺 高 优 秀 高 大 的 后 卫\", \"target_len\": 19, \"text_language\": \"Mandarin\"}\\n']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T01:55:27.968680Z",
     "start_time": "2024-08-27T01:55:27.290245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 随机读取几个文件检查\n",
    "import json\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD\"\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\"audio_datasets_2.jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print (file_path)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            print (lines[:10])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/test/audio_datasets_2.jsonl\n",
      "['{\"key\": \"1005608_42545eba\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005608/phase1/1005608_42545eba.wav\", \"source_len\": 469, \"target\": \"一 百 零 八 元 极 食 生 活 美 学 即 艺 术 美 食\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005610_4959cdde\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase1/1005610_4959cdde.wav\", \"source_len\": 731, \"target\": \"是 一 九 九 二 年 四 月 一 日 开 始 实 施 的\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005610_f637f4ff\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005610/phase1/1005610_f637f4ff.wav\", \"source_len\": 645, \"target\": \"创 造 出 独 一 无 二 的 地 道 澳 门 葡 国 菜\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005621_69034f9e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005621/phase1/1005621_69034f9e.wav\", \"source_len\": 767, \"target\": \"吃 蟹 时 和 吃 蟹 后 一 小 时 内 忌 饮 茶 水\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005647_4d5beaa1\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005647/phase1/1005647_4d5beaa1.wav\", \"source_len\": 645, \"target\": \"下 穿 蓝 色 小 短 裤 露 出 皮 带 显 得 时 尚 美 艳\", \"target_len\": 17, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005647_7e966afc\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005647/phase1/1005647_7e966afc.wav\", \"source_len\": 511, \"target\": \"不 久 后 亚 历 山 大 将 去 乌 克 兰 首 都 基 辅\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005647_d28e01c4\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005647/phase1/1005647_d28e01c4.wav\", \"source_len\": 551, \"target\": \"下 半 场 马 竞 整 体 压 上 的 很 凶\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005699_7860665a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005699/phase1/1005699_7860665a.wav\", \"source_len\": 589, \"target\": \"中 国 联 通 公 司 为 推 广 电 话 卡\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005716_7a2d3edb\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005716/phase1/1005716_7a2d3edb.wav\", \"source_len\": 747, \"target\": \"国 家 统 计 局 新 闻 发 言 人 盛 来 运 在 昨 天\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1005742_bedf0f6a\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1005742/phase1/1005742_bedf0f6a.wav\", \"source_len\": 493, \"target\": \"布 林 线 提 供 的 进 出 场 点 较 为 明 确\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n']\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/dev/audio_datasets_2.jsonl\n",
      "['{\"key\": \"1023032_ec212044\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023032/phase1/1023032_ec212044.wav\", \"source_len\": 579, \"target\": \"在 其 他 国 家 库 存 数 据 则 不 太 完 整\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023032_f50aeab2\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023032/phase1/1023032_f50aeab2.wav\", \"source_len\": 368, \"target\": \"在 你 看 来 可 不 是 那 么 一 回 事\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_16f0e3bd\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_16f0e3bd.wav\", \"source_len\": 509, \"target\": \"你 才 能 够 去 得 到 你 想 要 的 利 润\", \"target_len\": 13, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_2ccd7938\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_2ccd7938.wav\", \"source_len\": 471, \"target\": \"散 户 相 信 中 国 政 府 会 继 续 采 取 措 施\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_4f79ecf9\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_4f79ecf9.wav\", \"source_len\": 405, \"target\": \"除 沙 特 石 油 部 长 无 法 到 场 外\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_55a4f4d3\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_55a4f4d3.wav\", \"source_len\": 347, \"target\": \"比 如 阿 莫 西 林 或 头 孢 一 类 的\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_79ecb3c9\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_79ecb3c9.wav\", \"source_len\": 605, \"target\": \"德 翼 航 空 获 赔 三 亿 用 于 坠 机 赔 偿\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_d71ffc52\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_d71ffc52.wav\", \"source_len\": 597, \"target\": \"发 送 检 验 检 疫 机 构 指 定 的 邮 箱 中\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023127_e7676ff9\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023127/phase1/1023127_e7676ff9.wav\", \"source_len\": 459, \"target\": \"新 世 纪 等 强 队 也 非 常 有 实 力\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1023588_2a383d8f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1023588/phase1/1023588_2a383d8f.wav\", \"source_len\": 493, \"target\": \"以 前 羞 于 和 别 人 谈 论 过 去 的 事 情\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n']\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/train/audio_datasets_2.jsonl\n",
      "['{\"key\": \"1007380_706904a1\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_706904a1.wav\", \"source_len\": 458, \"target\": \"巡 展 组 委 会 通 过 车 展 中 国 的 平 台\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_8bc9430d\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_8bc9430d.wav\", \"source_len\": 825, \"target\": \"居 民 聚 集 地 等 环 境 敏 感 点 为 基 础\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_8cae8266\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_8cae8266.wav\", \"source_len\": 366, \"target\": \"工 作 九 年 的 人 资\", \"target_len\": 7, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_993adf51\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_993adf51.wav\", \"source_len\": 987, \"target\": \"哎 你 们 一 定 要 多 努 力 多 支 持 他 的 工 作\", \"target_len\": 16, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9c2c9d1f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9c2c9d1f.wav\", \"source_len\": 426, \"target\": \"工 作 人 员 正 在 对 供 暖 设 备 进 行 检 修\", \"target_len\": 15, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9f3dee01\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9f3dee01.wav\", \"source_len\": 511, \"target\": \"尽 管 海 军 定 期 举 行 类 似 演 练\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9faf9fce\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9faf9fce.wav\", \"source_len\": 415, \"target\": \"工 作 要 求 推 动 各 项 工 作 落 实\", \"target_len\": 12, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_9ff2071f\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_9ff2071f.wav\", \"source_len\": 159, \"target\": \"你 们 来 多 少 人\", \"target_len\": 6, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_a013c66e\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_a013c66e.wav\", \"source_len\": 529, \"target\": \"届 时 可 能 会 新 增 及 改 造 地 下 换 乘\", \"target_len\": 14, \"text_language\": \"Mandarin\"}\\n', '{\"key\": \"1007380_b006108c\", \"source\": \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1007380/phase1/1007380_b006108c.wav\", \"source_len\": 983, \"target\": \"尽 管 意 大 利 的 联 赛 里 不 缺 高 优 秀 高 大 的 后 卫\", \"target_len\": 19, \"text_language\": \"Mandarin\"}\\n']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T01:56:03.156780Z",
     "start_time": "2024-08-27T01:56:03.109185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把data4中WD里的所有audio_datasets_2.jsonl改成audio_datasets.jsonl\n",
    "import os\n",
    "\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD\"\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\"audio_datasets_2.jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            new_file_path = file_path.replace(\"audio_datasets_2\", \"audio_datasets\")\n",
    "            os.rename(file_path, new_file_path)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T03:37:19.818384Z",
     "start_time": "2024-08-30T03:37:18.279144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把data5中所有dev dataset中的test删掉\n",
    "\n",
    "test_json = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/test/audio_datasets.jsonl\"\n",
    "dev_json = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/dev/audio_datasets.jsonl\"\n",
    "\n",
    "with open(test_json, 'r') as f:\n",
    "    test_lines = f.readlines()\n",
    "    \n",
    "with open(dev_json, 'r') as f:\n",
    "    dev_lines = f.readlines()\n",
    "    \n",
    "a = 0\n",
    "for i in range(len(test_lines)):\n",
    "    if test_lines[i] in dev_lines:\n",
    "        dev_lines.remove(test_lines[i])\n",
    "        a += 1\n",
    "print (a)\n",
    "\n",
    "with open(dev_json, 'w') as f:\n",
    "    f.writelines(dev_lines)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14687\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T04:47:38.610238Z",
     "start_time": "2024-08-30T04:47:38.600412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 更新test5中的test\n",
    "import os\n",
    "import re\n",
    "root_dir = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech\"\n",
    "test_dir = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Tasks/SubdialectID/test_phase1/\"\n",
    "output_dir = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/test2/\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T04:47:40.070455Z",
     "start_time": "2024-08-30T04:47:40.062154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gather_wav_scp(wav_scp, dict):\n",
    "    with open(wav_scp, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, path = line.split(\" \")\n",
    "        path = os.path.join(root_dir, path)\n",
    "        if os.path.exists(path):\n",
    "            dict[id] = {}\n",
    "            dict[id].update({\"path\": path})\n",
    "    return dict\n",
    "\n",
    "def gether_dialect_info(utt2subdialect, dict):\n",
    "    with open(utt2subdialect, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, dialect = line.split(\" \")\n",
    "        dict[id].update({\"dialect\": dialect})\n",
    "    return dict\n",
    "\n",
    "def gather_text_info(text, dict):\n",
    "    with open(text, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, text = line.split(\" \", 1)\n",
    "        dict[id].update({\"text\": text})\n",
    "    return dict\n",
    "\n",
    "def contains_non_chinese(text):\n",
    "    # 正则表达式匹配非中文字符\n",
    "    non_chinese_pattern = re.compile(r'[^\\u4e00-\\u9fff]')\n",
    "    return non_chinese_pattern.search(text) is not None"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T04:47:41.738448Z",
     "start_time": "2024-08-30T04:47:41.325992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dict = {}\n",
    "\n",
    "test_wav_scp = os.path.join(test_dir, \"wav.scp\")\n",
    "test_utts2subdialect = os.path.join(test_dir, \"utt2subdialect\")\n",
    "test_text = os.path.join(test_dir, \"text\")\n",
    "test_dict = gather_wav_scp(test_wav_scp, test_dict)\n",
    "test_dict = gether_dialect_info(test_utts2subdialect, test_dict)\n",
    "test_dict = gather_text_info(test_text, test_dict)\n",
    "\n",
    "clean_test_dict = {}\n",
    "\n",
    "for id in test_dict.keys():\n",
    "    if contains_non_chinese(test_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_test_dict[id] = test_dict[id]\n",
    "\n",
    "\n",
    "test_dict = clean_test_dict\n",
    "\n",
    "print (\"Done!\")\n",
    "print (\"Test dataset size: \", len(test_dict))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Test dataset size:  44240\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T04:47:42.923333Z",
     "start_time": "2024-08-30T04:47:42.909891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, value in list(test_dict.items())[:10]:\n",
    "    print(key, value)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000043_20cfb79d {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_20cfb79d.wav', 'dialect': 'Mandarin', 'text': '参与游戏赢取现场抵用卷购车现金券'}\n",
      "1000043_2412bf4c {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_2412bf4c.wav', 'dialect': 'Mandarin', 'text': '又可以实现有害垃圾源头回收'}\n",
      "1000043_2e10312d {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_2e10312d.wav', 'dialect': 'Mandarin', 'text': '参加了的考试目前成绩也没有出来'}\n",
      "1000043_33fe2ef2 {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_33fe2ef2.wav', 'dialect': 'Mandarin', 'text': '参考书的阅读要花费较多的时间'}\n",
      "1000043_3c3d4a10 {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_3c3d4a10.wav', 'dialect': 'Mandarin', 'text': '参展企业可以将自己企业的新产品'}\n",
      "1000043_58fb23b5 {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_58fb23b5.wav', 'dialect': 'Northeastern', 'text': '参与风筝表演的都是世界级顶尖高手'}\n",
      "1000043_64cf3fc8 {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_64cf3fc8.wav', 'dialect': 'Mandarin', 'text': '又匆匆按计划去各景点玩游玩'}\n",
      "1000043_6674a7fd {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_6674a7fd.wav', 'dialect': 'Northeastern', 'text': '参与重大矛盾纠纷化解十一起'}\n",
      "1000043_671889da {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_671889da.wav', 'dialect': 'Mandarin', 'text': '反参透膜为核心的纯净水设备'}\n",
      "1000043_71dd2737 {'path': '/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Audio/1000043/phase1/1000043_71dd2737.wav', 'dialect': 'Northeastern', 'text': '反而在面颊间扫上淡淡的腮红'}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T04:47:45.649802Z",
     "start_time": "2024-08-30T04:47:45.562837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "wav_scp = os.path.join(output_dir, \"wav.scp\")\n",
    "text_path = os.path.join(output_dir, \"text\")\n",
    "\n",
    "with open(wav_scp, 'w') as f:\n",
    "    for id, info in test_dict.items():\n",
    "        path = info[\"path\"]\n",
    "        f.write(f\"{id} {path}\\n\")\n",
    "\n",
    "with open(text_path, 'w') as f:\n",
    "    for id, info in test_dict.items():\n",
    "        text = info[\"text\"]\n",
    "        f.write(f\"{id} {text}\\n\")\n",
    "\n",
    "print (\"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))\n",
    "            \n",
    "print (\"Done!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size:  44240 check True\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T04:50:01.765697Z",
     "start_time": "2024-08-30T04:50:01.722724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取口音文件并转换成字典\n",
    "dialect_root = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Tasks/SubdialectID/test_phase1/\"\n",
    "dialect_dict = {}\n",
    "for root, dirs, files in os.walk(dialect_root):\n",
    "    for file in files:\n",
    "        if file == \"utt2subdialect\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    id, dialect = line.split(\" \")\n",
    "                    dialect_dict[id] = dialect"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T04:50:03.200191Z",
     "start_time": "2024-08-30T04:50:03.189346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, value in list(dialect_dict.items())[:10]:\n",
    "    print(key, value)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000043_20cfb79d Mandarin\n",
      "1000043_2412bf4c Mandarin\n",
      "1000043_2e10312d Mandarin\n",
      "1000043_33fe2ef2 Mandarin\n",
      "1000043_3c3d4a10 Mandarin\n",
      "1000043_58fb23b5 Northeastern\n",
      "1000043_64cf3fc8 Mandarin\n",
      "1000043_6674a7fd Northeastern\n",
      "1000043_671889da Mandarin\n",
      "1000043_71dd2737 Northeastern\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T04:50:05.001262Z",
     "start_time": "2024-08-30T04:50:04.987976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_dialect(lines, dialect_dict, path):\n",
    "    \n",
    "    save_path = path.replace(\"audio_datasets\", \"audio_datasets2\")\n",
    "\n",
    "    import json\n",
    "    \n",
    "    updated_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # 解析 JSON 数据\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # 根据 key 在 dialect_dict 中查找对应的 dialect\n",
    "        key = data['key']\n",
    "        dialect = dialect_dict.get(key, \"Mandarin\")  # 如果 key 不存在于 dialect_dict 中，则默认值为 \"Unknown\"\n",
    "        \n",
    "        # 添加 \"dialect\" 字段\n",
    "        data['text_language'] = dialect\n",
    "        \n",
    "        # 将更新后的 JSON 数据转回字符串\n",
    "        updated_line = json.dumps(data, ensure_ascii=False) + '\\n'\n",
    "        updated_lines.append(updated_line)\n",
    "    \n",
    "    # 将更新后的内容写回文件\n",
    "    with open(save_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(updated_lines)\n",
    "        \n",
    "    os.remove(path)\n",
    "    os.renames(save_path, path)\n",
    "    \n",
    "    print(\"文件更新完成。\")"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T04:50:06.644292Z",
     "start_time": "2024-08-30T04:50:06.101152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/test2/audio_datasets.jsonl\"\n",
    "with open(data_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "add_dialect(lines, dialect_dict, data_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件更新完成。\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# data6\n",
    "## 把test data 从dev中删掉"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:23.325288Z",
     "start_time": "2024-08-28T08:43:23.317921Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:44.260038Z",
     "start_time": "2024-08-28T08:43:41.720765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_5 = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/\"\n",
    "data_6 = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data6/\"\n",
    "if not os.path.exists(data_6):\n",
    "    os.makedirs(data_6)\n",
    "\n",
    "# copy data2 to data4\n",
    "for root, dirs, files in os.walk(data_5):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        new_file_path = file_path.replace(\"data2\", \"data6\")\n",
    "        os.makedirs(os.path.dirname(new_file_path), exist_ok=True)\n",
    "        os.system(f\"cp {file_path} {new_file_path}\")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:47.220095Z",
     "start_time": "2024-08-28T08:43:47.155976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把dev中的test数据删掉\n",
    "import json\n",
    "\n",
    "test_json = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data6/WD/test/audio_datasets.jsonl\"\n",
    "dev_json = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data6/WD/dev/audio_datasets.jsonl\"\n",
    "\n",
    "with open(test_json, 'r') as f:\n",
    "    test_lines = f.readlines()\n",
    "    \n",
    "with open(dev_json, 'r') as f:\n",
    "    dev_lines = f.readlines()\n",
    "    \n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:53.556972Z",
     "start_time": "2024-08-28T08:43:50.506613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = 0\n",
    "for i in range(len(test_lines)):\n",
    "    if test_lines[i] in dev_lines:\n",
    "        dev_lines.remove(test_lines[i])\n",
    "        a += 1\n",
    "print (a)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14687\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:58.436647Z",
     "start_time": "2024-08-28T08:43:58.422964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存dev\n",
    "with open(dev_json, 'w') as f:\n",
    "    f.writelines(dev_lines)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T09:20:19.376549Z",
     "start_time": "2024-09-03T09:20:19.373968Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T09:23:05.795677Z",
     "start_time": "2024-09-03T09:23:05.671893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把data5中的test的asr-ar的数据集中的beijing和northeastern删掉\n",
    "import json\n",
    "ori_path = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/test/audio_datasets.jsonl\"\n",
    "new_path = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data5/WD/test/audio_datasets2.jsonl\"\n",
    "with open(ori_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    if data[\"text_language\"] == \"Beijing\" or data[\"text_language\"] == \"Northeastern\":\n",
    "        continue\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "\n",
    "with open(new_path, 'w') as f:\n",
    "    f.writelines(new_lines)\n",
    "    \n",
    "print (\"Done!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funasr2024_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
