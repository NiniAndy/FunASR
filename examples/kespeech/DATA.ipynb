{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理KeSpeech-ASR数据集\n",
    "\n",
    "|- Each_subdialect    \n",
    "&emsp;|- Mandarin   \n",
    "&emsp;|- BJ   \n",
    "&emsp;|- SW   \n",
    "&emsp;|- ZY    \n",
    "&emsp;|- NE   \n",
    "&emsp;|- LY  \n",
    "&emsp;|- JH    \n",
    "&emsp;|- JLu   \n",
    "&emsp;|- JLo   \n",
    "|- All_subdialect   \n",
    "|- Mandarin  \n",
    "|-Whole_training_set  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "root_dir = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech\"\n",
    "audio_dir = os.path.join(root_dir, \"Audio\")\n",
    "asr_text_dir = os.path.join(root_dir, \"Tasks/ASR\")\n",
    "train_phase_1_dir = os.path.join(asr_text_dir, \"train_phase1\")\n",
    "train_phase_2_dir = os.path.join(asr_text_dir, \"train_phase2\")\n",
    "train_list = [train_phase_1_dir, train_phase_2_dir]\n",
    "dev_phase_1_dir = os.path.join(asr_text_dir, \"dev_phase1\")\n",
    "dev_phase_2_dir = os.path.join(asr_text_dir, \"dev_phase2\")\n",
    "dev_list = [dev_phase_1_dir, dev_phase_2_dir]\n",
    "test_dir = os.path.join(asr_text_dir, \"test\")\n",
    "\n",
    "output_dir = \"/ssd/zhuang/code/FunASR2024/examples/kespeech/DATA/data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_wav_scp(wav_scp, dict):\n",
    "    with open(wav_scp, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, path = line.split(\" \")\n",
    "        path = os.path.join(root_dir, path)\n",
    "        if os.path.exists(path):\n",
    "            dict[id] = {}\n",
    "            dict[id].update({\"path\": path})\n",
    "    return dict\n",
    "\n",
    "def gether_dialect_info(utt2subdialect, dict):\n",
    "    with open(utt2subdialect, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, dialect = line.split(\" \")\n",
    "        dict[id].update({\"dialect\": dialect})\n",
    "    return dict\n",
    "\n",
    "def gather_text_info(text, dict):\n",
    "    with open(text, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, text = line.split(\" \", 1)\n",
    "        dict[id].update({\"text\": text})\n",
    "    return dict\n",
    "\n",
    "def contains_non_chinese(text):\n",
    "    # 正则表达式匹配非中文字符\n",
    "    non_chinese_pattern = re.compile(r'[^\\u4e00-\\u9fff]')\n",
    "    return non_chinese_pattern.search(text) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Train dataset size:  881808\n",
      "Dev dataset size:  4399\n",
      "Test dataset size:  19668\n"
     ]
    }
   ],
   "source": [
    "train_dict  = {}\n",
    "dev_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "for train_path in train_list:\n",
    "    train_wav_scp = os.path.join(train_path, \"wav.scp\")\n",
    "    train_utts2subdialect = os.path.join(train_path, \"utt2subdialect\")\n",
    "    traain_text = os.path.join(train_path, \"text\")\n",
    "    train_dict = gather_wav_scp(train_wav_scp, train_dict)\n",
    "    train_dict = gether_dialect_info(train_utts2subdialect, train_dict)\n",
    "    train_dict = gather_text_info(traain_text, train_dict)\n",
    "for dev_path in dev_list:\n",
    "    dev_wav_scp = os.path.join(dev_path, \"wav.scp\")\n",
    "    dev_utts2subdialect = os.path.join(dev_path, \"utt2subdialect\")\n",
    "    dev_text = os.path.join(dev_path, \"text\")\n",
    "    dev_dict = gather_wav_scp(dev_wav_scp, dev_dict)\n",
    "    dev_dict = gether_dialect_info(dev_utts2subdialect, dev_dict)\n",
    "    dev_dict = gather_text_info(dev_text, dev_dict)\n",
    "\n",
    "test_wav_scp = os.path.join(test_dir, \"wav.scp\")\n",
    "test_utts2subdialect = os.path.join(test_dir, \"utt2subdialect\")\n",
    "test_text = os.path.join(test_dir, \"text\")\n",
    "test_dict = gather_wav_scp(test_wav_scp, test_dict)\n",
    "test_dict = gether_dialect_info(test_utts2subdialect, test_dict)\n",
    "test_dict = gather_text_info(test_text, test_dict)\n",
    "\n",
    "clean_train_dict = {}\n",
    "clean_dev_dict = {}\n",
    "clean_test_dict = {}\n",
    "\n",
    "for id in train_dict.keys():\n",
    "    if contains_non_chinese(train_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_train_dict[id] = train_dict[id]\n",
    "for id in dev_dict.keys():\n",
    "    if contains_non_chinese(dev_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_dev_dict[id] = dev_dict[id]\n",
    "for id in test_dict.keys():\n",
    "    if contains_non_chinese(test_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_test_dict[id] = test_dict[id]\n",
    "\n",
    "train_dict = clean_train_dict\n",
    "dev_dict = clean_dev_dict\n",
    "test_dict = clean_test_dict\n",
    "\n",
    "\n",
    "print (\"Done!\")\n",
    "print (\"Train dataset size: \", len(train_dict))\n",
    "print (\"Dev dataset size: \", len(dev_dict))\n",
    "print (\"Test dataset size: \", len(test_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each_subdialect   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_subdialect_dir =  os.path.join(output_dir, \"ES\")\n",
    "os.makedirs(each_subdialect_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Make wav.scp and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin train size:  678515 check True\n",
      "Mandarin dev size:  3681 check True\n",
      "Mandarin test size:  4981 check True\n",
      "Done!\n",
      "Beijing train size:  2237 check True\n",
      "Beijing dev size:  31 check True\n",
      "Beijing test size:  265 check True\n",
      "Done!\n",
      "Southwestern train size:  45359 check True\n",
      "Southwestern dev size:  132 check True\n",
      "Southwestern test size:  2684 check True\n",
      "Done!\n",
      "Jiao-Liao train size:  20268 check True\n",
      "Jiao-Liao dev size:  118 check True\n",
      "Jiao-Liao test size:  1443 check True\n",
      "Done!\n",
      "Northeastern train size:  4843 check True\n",
      "Northeastern dev size:  5 check True\n",
      "Northeastern test size:  350 check True\n",
      "Done!\n",
      "Jiang-Huai train size:  27586 check True\n",
      "Jiang-Huai dev size:  105 check True\n",
      "Jiang-Huai test size:  2268 check True\n",
      "Done!\n",
      "Lan-Yin train size:  20549 check True\n",
      "Lan-Yin dev size:  104 check True\n",
      "Lan-Yin test size:  1646 check True\n",
      "Done!\n",
      "Ji-Lu train size:  33861 check True\n",
      "Ji-Lu dev size:  156 check True\n",
      "Ji-Lu test size:  2806 check True\n",
      "Done!\n",
      "Zhongyuan train size:  48590 check True\n",
      "Zhongyuan dev size:  67 check True\n",
      "Zhongyuan test size:  3225 check True\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Mandarin', 'Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "for dialect in subdialect_list:\n",
    "    es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "    os.makedirs(es_subdialect_dir, exist_ok=True)\n",
    "\n",
    "    es_subdialect_train = os.path.join(es_subdialect_dir, \"train\")\n",
    "    es_subdialect_dev = os.path.join(es_subdialect_dir, \"dev\")\n",
    "    es_subdialect_test = os.path.join(es_subdialect_dir, \"test\")\n",
    "\n",
    "    es_subdialect_output = [es_subdialect_train, es_subdialect_dev, es_subdialect_test]\n",
    "    data_dict = [train_dict, dev_dict, test_dict]\n",
    "\n",
    "    for output, dict in zip(es_subdialect_output, data_dict):\n",
    "        os.makedirs(output, exist_ok=True)\n",
    "        wav_scp = os.path.join(output, \"wav.scp\")\n",
    "        text_path = os.path.join(output, \"text\")\n",
    "\n",
    "        with open(wav_scp, 'w') as f:\n",
    "            for id, info in dict.items():\n",
    "                if info[\"dialect\"] == dialect:\n",
    "                    path = info[\"path\"]\n",
    "                    f.write(f\"{id} {path}\\n\")\n",
    "\n",
    "        with open(text_path, 'w') as f:\n",
    "            for id, info in dict.items():\n",
    "                if info[\"dialect\"] == dialect:\n",
    "                    text = info[\"text\"]\n",
    "                    f.write(f\"{id} {text}\\n\")\n",
    "\n",
    "        print (dialect, output.split(\"/\")[-1], \"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))\n",
    "            \n",
    "    print (\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the test to the dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(read_file, writen_file):\n",
    "    # 打开第一个文件以读取数据\n",
    "    with open(read_file, 'r', encoding='utf-8') as f1:\n",
    "        data = f1.read()  # 读取全部内容\n",
    "\n",
    "    # 打开第二个文件以追加数据\n",
    "    with open(writen_file, 'a', encoding='utf-8') as f2:\n",
    "        f2.write(data)  # 将读取的数据追加到文件末尾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing Dev dataset oringinal size:  31 = 31\n",
      "Beijing Test dataset oringinal size:  265 = 265\n",
      "Beijing Dev dataset size:  296 = 296\n",
      "Beijing Test dataset size:  265 = 265\n",
      "\n",
      "\n",
      "Southwestern Dev dataset oringinal size:  132 = 132\n",
      "Southwestern Test dataset oringinal size:  2684 = 2684\n",
      "Southwestern Dev dataset size:  2816 = 2816\n",
      "Southwestern Test dataset size:  2684 = 2684\n",
      "\n",
      "\n",
      "Jiao-Liao Dev dataset oringinal size:  118 = 118\n",
      "Jiao-Liao Test dataset oringinal size:  1443 = 1443\n",
      "Jiao-Liao Dev dataset size:  1561 = 1561\n",
      "Jiao-Liao Test dataset size:  1443 = 1443\n",
      "\n",
      "\n",
      "Northeastern Dev dataset oringinal size:  5 = 5\n",
      "Northeastern Test dataset oringinal size:  350 = 350\n",
      "Northeastern Dev dataset size:  355 = 355\n",
      "Northeastern Test dataset size:  350 = 350\n",
      "\n",
      "\n",
      "Jiang-Huai Dev dataset oringinal size:  105 = 105\n",
      "Jiang-Huai Test dataset oringinal size:  2268 = 2268\n",
      "Jiang-Huai Dev dataset size:  2373 = 2373\n",
      "Jiang-Huai Test dataset size:  2268 = 2268\n",
      "\n",
      "\n",
      "Lan-Yin Dev dataset oringinal size:  104 = 104\n",
      "Lan-Yin Test dataset oringinal size:  1646 = 1646\n",
      "Lan-Yin Dev dataset size:  1750 = 1750\n",
      "Lan-Yin Test dataset size:  1646 = 1646\n",
      "\n",
      "\n",
      "Ji-Lu Dev dataset oringinal size:  156 = 156\n",
      "Ji-Lu Test dataset oringinal size:  2806 = 2806\n",
      "Ji-Lu Dev dataset size:  2962 = 2962\n",
      "Ji-Lu Test dataset size:  2806 = 2806\n",
      "\n",
      "\n",
      "Zhongyuan Dev dataset oringinal size:  67 = 67\n",
      "Zhongyuan Test dataset oringinal size:  3225 = 3225\n",
      "Zhongyuan Dev dataset size:  3292 = 3292\n",
      "Zhongyuan Test dataset size:  3225 = 3225\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "for dialect in subdialect_list:\n",
    "    \n",
    "    es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "    es_subdialect_dev = os.path.join(es_subdialect_dir, \"dev\")\n",
    "    es_subdialect_test = os.path.join(es_subdialect_dir, \"test\")\n",
    "\n",
    "    es_subdialect_dev_wav_scp = os.path.join(es_subdialect_dev, \"wav.scp\")\n",
    "    es_subdialect_dev_text = os.path.join(es_subdialect_dev, \"text\")\n",
    "\n",
    "    es_subdialect_test_wav_scp = os.path.join(es_subdialect_test, \"wav.scp\")\n",
    "    es_subdialect_test_text = os.path.join(es_subdialect_test, \"text\")\n",
    "\n",
    "    print (dialect, \"Dev dataset oringinal size: \", len(open(es_subdialect_dev_wav_scp).readlines()), \"=\", len(open(es_subdialect_dev_text).readlines()))\n",
    "    print (dialect, \"Test dataset oringinal size: \", len(open(es_subdialect_test_wav_scp).readlines()), \"=\", len(open(es_subdialect_test_text).readlines()))\n",
    "\n",
    "    merge_files(es_subdialect_test_wav_scp, es_subdialect_dev_wav_scp)\n",
    "    merge_files(es_subdialect_test_text, es_subdialect_dev_text)\n",
    "\n",
    "    print (dialect, \"Dev dataset size: \", len(open(es_subdialect_dev_wav_scp).readlines()), \"=\", len(open(es_subdialect_dev_text).readlines()))\n",
    "    print (dialect, \"Test dataset size: \", len(open(es_subdialect_test_wav_scp).readlines()),\"=\", len(open(es_subdialect_test_text).readlines()))\n",
    "\n",
    "    print (\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All_subdialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  203293 check True\n",
      "dev size:  15405 check True\n",
      "test size:  14687 check True\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "\n",
    "all_subdialect_dir = os.path.join(output_dir, \"AS\")\n",
    "os.makedirs(all_subdialect_dir, exist_ok=True)\n",
    "all_subdialect_train = os.path.join(all_subdialect_dir, \"train\")\n",
    "WholeDevSet = os.path.join(all_subdialect_dir, \"dev\")\n",
    "WholeTestSet = os.path.join(all_subdialect_dir, \"test\")\n",
    "whole_dataset_output = [all_subdialect_train, WholeDevSet, WholeTestSet]\n",
    "\n",
    "for output in whole_dataset_output:\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    wav_scp = os.path.join(output, \"wav.scp\")\n",
    "    text_path = os.path.join(output, \"text\")\n",
    "\n",
    "    with open(wav_scp, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_wav_scp = os.path.join(es_subdialect_output, \"wav.scp\")\n",
    "            for line in open(es_subdialect_wav_scp):\n",
    "                f.write(line)\n",
    "\n",
    "    with open(text_path, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_text = os.path.join(es_subdialect_output, \"text\")\n",
    "            for line in open(es_subdialect_text):\n",
    "                f.write(line)\n",
    "\n",
    "    print (output.split(\"/\")[-1], \"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  881808 check True\n",
      "dev size:  19086 check True\n",
      "test size:  19668 check True\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Mandarin', 'Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "\n",
    "all_subdialect_dir = os.path.join(output_dir, \"WD\")\n",
    "os.makedirs(all_subdialect_dir, exist_ok=True)\n",
    "all_subdialect_train = os.path.join(all_subdialect_dir, \"train\")\n",
    "WholeDevSet = os.path.join(all_subdialect_dir, \"dev\")\n",
    "WholeTestSet = os.path.join(all_subdialect_dir, \"test\")\n",
    "whole_dataset_output = [all_subdialect_train, WholeDevSet, WholeTestSet]\n",
    "\n",
    "for output in whole_dataset_output:\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    wav_scp = os.path.join(output, \"wav.scp\")\n",
    "    text_path = os.path.join(output, \"text\")\n",
    "\n",
    "    with open(wav_scp, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_wav_scp = os.path.join(es_subdialect_output, \"wav.scp\")\n",
    "            for line in open(es_subdialect_wav_scp):\n",
    "                f.write(line)\n",
    "\n",
    "    with open(text_path, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_text = os.path.join(es_subdialect_output, \"text\")\n",
    "            for line in open(es_subdialect_text):\n",
    "                f.write(line)\n",
    "\n",
    "    print (output.split(\"/\")[-1], \"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After CNAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(read_file, writen_file):\n",
    "    # 打开第一个文件以读取数据\n",
    "    with open(read_file, 'r', encoding='utf-8') as f1:\n",
    "        data = f1.read()  # 读取全部内容\n",
    "\n",
    "    # 打开第二个文件以追加数据\n",
    "    with open(writen_file, 'a', encoding='utf-8') as f2:\n",
    "        f2.write(data)  # 将读取的数据追加到文件末尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing train dataset size:  2237\n",
      "Southwestern train dataset size:  45359\n",
      "Jiao-Liao train dataset size:  20268\n",
      "Northeastern train dataset size:  4843\n",
      "Jiang-Huai train dataset size:  27586\n",
      "Lan-Yin train dataset size:  20549\n",
      "Ji-Lu train dataset size:  33861\n",
      "Zhongyuan train dataset size:  48590\n",
      "train dataset size:  203293\n",
      "Beijing dev dataset size:  296\n",
      "Southwestern dev dataset size:  2816\n",
      "Jiao-Liao dev dataset size:  1561\n",
      "Northeastern dev dataset size:  355\n",
      "Jiang-Huai dev dataset size:  2373\n",
      "Lan-Yin dev dataset size:  1750\n",
      "Ji-Lu dev dataset size:  2962\n",
      "Zhongyuan dev dataset size:  3292\n",
      "dev dataset size:  15405\n",
      "Beijing test dataset size:  265\n",
      "Southwestern test dataset size:  2684\n",
      "Jiao-Liao test dataset size:  1443\n",
      "Northeastern test dataset size:  350\n",
      "Jiang-Huai test dataset size:  2268\n",
      "Lan-Yin test dataset size:  1646\n",
      "Ji-Lu test dataset size:  2806\n",
      "Zhongyuan test dataset size:  3225\n",
      "test dataset size:  14687\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "section_list = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "all_subdialect_dir = os.path.join(output_dir, \"AS\")\n",
    "each_subdialect_dir = os.path.join(output_dir, \"ES\")\n",
    "\n",
    "\n",
    "for section in section_list:\n",
    "    all_subdialect_section = os.path.join(all_subdialect_dir, section)\n",
    "    all_subdialect_section_audio_datasets = os.path.join(all_subdialect_section, \"audio_datasets.jsonl\")\n",
    "    os.makedirs(all_subdialect_section, exist_ok=True)\n",
    "\n",
    "    for dialect in subdialect_list:\n",
    "        es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "        es_subdialect_section = os.path.join(es_subdialect_dir, section)\n",
    "        es_audio_datasets  = os.path.join(es_subdialect_section, \"audio_datasets.jsonl\")\n",
    "        print (dialect, section, \"dataset size: \", len(open(es_audio_datasets).readlines()))\n",
    "        merge_files(es_audio_datasets, all_subdialect_section_audio_datasets)\n",
    "\n",
    "    print (section, \"dataset size: \", len(open(all_subdialect_section_audio_datasets).readlines()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin train dataset size:  678515\n",
      "Beijing train dataset size:  2237\n",
      "Southwestern train dataset size:  45359\n",
      "Jiao-Liao train dataset size:  20268\n",
      "Northeastern train dataset size:  4843\n",
      "Jiang-Huai train dataset size:  27586\n",
      "Lan-Yin train dataset size:  20549\n",
      "Ji-Lu train dataset size:  33861\n",
      "Zhongyuan train dataset size:  48590\n",
      "train dataset size:  881808\n",
      "Mandarin dev dataset size:  3681\n",
      "Beijing dev dataset size:  296\n",
      "Southwestern dev dataset size:  2816\n",
      "Jiao-Liao dev dataset size:  1561\n",
      "Northeastern dev dataset size:  355\n",
      "Jiang-Huai dev dataset size:  2373\n",
      "Lan-Yin dev dataset size:  1750\n",
      "Ji-Lu dev dataset size:  2962\n",
      "Zhongyuan dev dataset size:  3292\n",
      "dev dataset size:  19086\n",
      "Mandarin test dataset size:  4981\n",
      "Beijing test dataset size:  265\n",
      "Southwestern test dataset size:  2684\n",
      "Jiao-Liao test dataset size:  1443\n",
      "Northeastern test dataset size:  350\n",
      "Jiang-Huai test dataset size:  2268\n",
      "Lan-Yin test dataset size:  1646\n",
      "Ji-Lu test dataset size:  2806\n",
      "Zhongyuan test dataset size:  3225\n",
      "test dataset size:  19668\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Mandarin', 'Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "section_list = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "whole_dir = os.path.join(output_dir, \"WD\")\n",
    "each_subdialect_dir = os.path.join(output_dir, \"ES\")\n",
    "\n",
    "for section in section_list:\n",
    "    whole_section = os.path.join(whole_dir, section)\n",
    "    whole_audio_datasets = os.path.join(whole_section, \"audio_datasets.jsonl\")\n",
    "    os.makedirs(whole_section, exist_ok=True)\n",
    "\n",
    "    for dialect in subdialect_list:\n",
    "        es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "        es_subdialect_section = os.path.join(es_subdialect_dir, section)\n",
    "        es_audio_datasets  = os.path.join(es_subdialect_section, \"audio_datasets.jsonl\")\n",
    "        print (dialect, section, \"dataset size: \", len(open(es_audio_datasets).readlines()))\n",
    "        merge_files(es_audio_datasets, whole_audio_datasets)\n",
    "\n",
    "    print (section, \"dataset size: \", len(open(whole_audio_datasets).readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 添加口音信息"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.162177Z",
     "start_time": "2024-07-29T02:47:39.185234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 源文件夹路径\n",
    "source_dir = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data\"\n",
    "# 目标文件夹路径\n",
    "data_root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2\"\n",
    "\n",
    "# 确保目标目录存在，如果不存在则创建\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "# 使用shutil.copytree来复制目录。从 Python 3.8 开始，copytree() 可以接受 dirs_exist_ok 参数\n",
    "# 如果目标目录已存在，并且你想要覆盖里面的文件，则需要设置 dirs_exist_ok=True\n",
    "try:\n",
    "    shutil.copytree(source_dir, data_root, dirs_exist_ok=True)\n",
    "except FileExistsError:\n",
    "    # 如果在较早的 Python 版本中使用 shutil.copytree 且目标目录已存在，会引发此错误\n",
    "    print(\"目录已存在\")\n",
    "except Exception as e:\n",
    "    print(f\"复制过程中出错: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.172035Z",
     "start_time": "2024-07-29T02:47:40.164904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "end_name = \"audio_datasets.jsonl\"\n",
    "\n",
    "# 遍历所有的.jsonl文件\n",
    "data_list = []\n",
    "for root, dirs, files in os.walk(data_root):\n",
    "    for file in files:\n",
    "        if file == end_name:\n",
    "            file_path = os.path.join(root, file)\n",
    "            data_list.append(file_path)\n",
    "            \n",
    "print (data_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/train/audio_datasets.jsonl']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.482450Z",
     "start_time": "2024-07-29T02:47:40.173550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取口音文件并转换成字典\n",
    "dialect_root = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Tasks/SubdialectID\"\n",
    "dialect_dict = {}\n",
    "for root, dirs, files in os.walk(dialect_root):\n",
    "    for file in files:\n",
    "        if file == \"utt2subdialect\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    id, dialect = line.split(\" \")\n",
    "                    dialect_dict[id] = dialect"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.489388Z",
     "start_time": "2024-07-29T02:47:40.484407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_dialect(lines, dialect_dict, path):\n",
    "    \n",
    "    save_path = path.replace(\"audio_datasets\", \"audio_datasets2\")\n",
    "\n",
    "    import json\n",
    "    \n",
    "    updated_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # 解析 JSON 数据\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # 根据 key 在 dialect_dict 中查找对应的 dialect\n",
    "        key = data['key']\n",
    "        dialect = dialect_dict.get(key, \"Mandarin\")  # 如果 key 不存在于 dialect_dict 中，则默认值为 \"Unknown\"\n",
    "        \n",
    "        # 添加 \"dialect\" 字段\n",
    "        data['text_language'] = dialect\n",
    "        \n",
    "        # 将更新后的 JSON 数据转回字符串\n",
    "        updated_line = json.dumps(data, ensure_ascii=False) + '\\n'\n",
    "        updated_lines.append(updated_line)\n",
    "    \n",
    "    # 将更新后的内容写回文件\n",
    "    with open(save_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(updated_lines)\n",
    "        \n",
    "    os.remove(path)\n",
    "    os.renames(save_path, path)\n",
    "    \n",
    "    print(\"文件更新完成。\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:47:40.568448Z",
     "start_time": "2024-07-29T02:47:40.490438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_10_items = list(dialect_dict.items())[:10]\n",
    "\n",
    "# 打印前10个项目\n",
    "for key, value in first_10_items:\n",
    "    print(f'{key}: {value}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000001_0b1a33a3: Mandarin\n",
      "1000001_0e9793ff: Mandarin\n",
      "1000001_11f3978b: Mandarin\n",
      "1000001_1c4b6ce5: Mandarin\n",
      "1000001_2c863844: Mandarin\n",
      "1000001_3c84b37d: Mandarin\n",
      "1000001_492740a5: Mandarin\n",
      "1000001_5c8b5985: Mandarin\n",
      "1000001_63740949: Mandarin\n",
      "1000001_6a7435f1: Mandarin\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.429008Z",
     "start_time": "2024-07-29T02:47:40.569576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(data_list)):\n",
    "# for i in range (1):\n",
    "    data_path = data_list[i]\n",
    "    print (\"Processing: \", data_path)\n",
    "    with open(data_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    add_dialect(lines, dialect_dict, data_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/AS/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/WD/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiang-Huai/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Lan-Yin/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Northeastern/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Southwestern/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Ji-Lu/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Zhongyuan/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Beijing/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Mandarin/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/ES/Jiao-Liao/train/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/test/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/dev/audio_datasets.jsonl\n",
      "文件更新完成。\n",
      "Processing:  /ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/MD/train/audio_datasets.jsonl\n",
      "文件更新完成。\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 新建口音字典"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.434507Z",
     "start_time": "2024-07-29T02:48:05.430618Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.665939Z",
     "start_time": "2024-07-29T02:48:05.435822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取口音文件并转换成字典\n",
    "dialect_root = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech/Tasks/SubdialectID\"\n",
    "dialect_list = []\n",
    "for root, dirs, files in os.walk(dialect_root):\n",
    "    for file in files:\n",
    "        if file == \"utt2subdialect\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    id, dialect = line.split(\" \")\n",
    "                    if dialect not in dialect_list:\n",
    "                        dialect_list.append(dialect)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.670980Z",
     "start_time": "2024-07-29T02:48:05.667290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_dialect_path = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data2/zh_token_list/char/dialects.txt\"\n",
    "with open(save_dialect_path, 'w') as f:\n",
    "    for dialect in dialect_list:\n",
    "        f.write(f\"{dialect}\\n\")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T02:48:05.675516Z",
     "start_time": "2024-07-29T02:48:05.673288Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 将重复的内容清洗掉"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T03:55:05.608895Z",
     "start_time": "2024-07-31T03:55:05.598210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T03:56:09.422581Z",
     "start_time": "2024-07-31T03:56:09.410231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pending_list = []\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            pending_list.append(file_path)\n",
    "print (pending_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.bac.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.bac.jsonl', '/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.jsonl']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T03:57:12.462089Z",
     "start_time": "2024-07-31T03:57:12.450986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    print (file_path, \"size: \", len(lines))\n",
    "    lines = list(set(lines))\n",
    "    print (file_path, \"size: \", len(lines))\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.writelines(lines)\n",
    "    print (file_path, \"cleaned!\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T03:57:19.899621Z",
     "start_time": "2024-07-31T03:57:16.627782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file_path in pending_list:\n",
    "    clean_file(file_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets.jsonl size:  2268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets.jsonl size:  2268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets.jsonl size:  2373\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets.jsonl size:  2373\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets.jsonl size:  27586\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets.jsonl size:  27586\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiang-Huai/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets.jsonl size:  1646\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets.jsonl size:  1646\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets.jsonl size:  1750\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets.jsonl size:  1750\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets.jsonl size:  20549\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets.jsonl size:  20549\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Lan-Yin/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets.jsonl size:  350\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets.jsonl size:  350\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets.jsonl size:  355\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets.jsonl size:  355\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets.jsonl size:  4843\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets.jsonl size:  4843\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Northeastern/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets.jsonl size:  2684\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets.jsonl size:  2684\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets.jsonl size:  2816\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets.jsonl size:  2816\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets.jsonl size:  45359\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets.jsonl size:  45359\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Southwestern/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets.jsonl size:  2806\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets.jsonl size:  2806\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets.jsonl size:  2962\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets.jsonl size:  2962\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.bac.jsonl size:  33861\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.bac.jsonl size:  33861\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.bac.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.jsonl size:  101583\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.jsonl size:  33861\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Ji-Lu/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets.jsonl size:  3225\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets.jsonl size:  3225\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets.jsonl size:  3292\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets.jsonl size:  3292\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets.jsonl size:  48590\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets.jsonl size:  48590\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Zhongyuan/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets.jsonl size:  265\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets.jsonl size:  265\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets.jsonl size:  296\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets.jsonl size:  296\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets.jsonl size:  2237\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets.jsonl size:  2237\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Beijing/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets.jsonl size:  4981\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets.jsonl size:  4981\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets.jsonl size:  3681\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets.jsonl size:  3681\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets.jsonl size:  678515\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets.jsonl size:  678515\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Mandarin/train/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets.jsonl size:  1443\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets.jsonl size:  1443\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/test/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets.jsonl size:  1561\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets.jsonl size:  1561\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/dev/audio_datasets.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.bac.jsonl size:  20268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.bac.jsonl size:  20268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.bac.jsonl cleaned!\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.jsonl size:  101340\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.jsonl size:  20268\n",
      "/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/Jiao-Liao/train/audio_datasets.jsonl cleaned!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T04:04:57.736023Z",
     "start_time": "2024-07-31T04:04:45.750347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "root = \"/ssd/zhuang/code/FunASR/examples/kespeech/DATA/data3/ES/\"\n",
    "\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        if \"bac\" in file:\n",
    "            print (file)\n",
    "            os.remove(os.path.join(root, file))\n",
    "            print (file, \"removed!\")\n",
    "        if \"hubert\" in file:\n",
    "            print (file)\n",
    "            os.remove(os.path.join(root, file))\n",
    "            print (file, \"removed!\")\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "hubert_9layer.npy\n",
      "hubert_9layer.npy removed!\n",
      "hubert_9layer.len\n",
      "hubert_9layer.len removed!\n",
      "audio_datasets.bac.jsonl\n",
      "audio_datasets.bac.jsonl removed!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funasr2024_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
