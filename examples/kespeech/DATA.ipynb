{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理KeSpeech-ASR数据集\n",
    "\n",
    "|- Each_subdialect    \n",
    "&emsp;|- Mandarin   \n",
    "&emsp;|- BJ   \n",
    "&emsp;|- SW   \n",
    "&emsp;|- ZY    \n",
    "&emsp;|- NE   \n",
    "&emsp;|- LY  \n",
    "&emsp;|- JH    \n",
    "&emsp;|- JLu   \n",
    "&emsp;|- JLo   \n",
    "|- All_subdialect   \n",
    "|- Mandarin  \n",
    "|-Whole_training_set  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "root_dir = \"/ssd/zhuang/dataset/data_KeSpeech/KeSpeech\"\n",
    "audio_dir = os.path.join(root_dir, \"Audio\")\n",
    "asr_text_dir = os.path.join(root_dir, \"Tasks/ASR\")\n",
    "train_phase_1_dir = os.path.join(asr_text_dir, \"train_phase1\")\n",
    "train_phase_2_dir = os.path.join(asr_text_dir, \"train_phase2\")\n",
    "train_list = [train_phase_1_dir, train_phase_2_dir]\n",
    "dev_phase_1_dir = os.path.join(asr_text_dir, \"dev_phase1\")\n",
    "dev_phase_2_dir = os.path.join(asr_text_dir, \"dev_phase2\")\n",
    "dev_list = [dev_phase_1_dir, dev_phase_2_dir]\n",
    "test_dir = os.path.join(asr_text_dir, \"test\")\n",
    "\n",
    "output_dir = \"/ssd/zhuang/code/FunASR2024/examples/kespeech/DATA/data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_wav_scp(wav_scp, dict):\n",
    "    with open(wav_scp, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, path = line.split(\" \")\n",
    "        path = os.path.join(root_dir, path)\n",
    "        if os.path.exists(path):\n",
    "            dict[id] = {}\n",
    "            dict[id].update({\"path\": path})\n",
    "    return dict\n",
    "\n",
    "def gether_dialect_info(utt2subdialect, dict):\n",
    "    with open(utt2subdialect, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, dialect = line.split(\" \")\n",
    "        dict[id].update({\"dialect\": dialect})\n",
    "    return dict\n",
    "\n",
    "def gather_text_info(text, dict):\n",
    "    with open(text, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        id, text = line.split(\" \", 1)\n",
    "        dict[id].update({\"text\": text})\n",
    "    return dict\n",
    "\n",
    "def contains_non_chinese(text):\n",
    "    # 正则表达式匹配非中文字符\n",
    "    non_chinese_pattern = re.compile(r'[^\\u4e00-\\u9fff]')\n",
    "    return non_chinese_pattern.search(text) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Train dataset size:  881808\n",
      "Dev dataset size:  4399\n",
      "Test dataset size:  19668\n"
     ]
    }
   ],
   "source": [
    "train_dict  = {}\n",
    "dev_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "for train_path in train_list:\n",
    "    train_wav_scp = os.path.join(train_path, \"wav.scp\")\n",
    "    train_utts2subdialect = os.path.join(train_path, \"utt2subdialect\")\n",
    "    traain_text = os.path.join(train_path, \"text\")\n",
    "    train_dict = gather_wav_scp(train_wav_scp, train_dict)\n",
    "    train_dict = gether_dialect_info(train_utts2subdialect, train_dict)\n",
    "    train_dict = gather_text_info(traain_text, train_dict)\n",
    "for dev_path in dev_list:\n",
    "    dev_wav_scp = os.path.join(dev_path, \"wav.scp\")\n",
    "    dev_utts2subdialect = os.path.join(dev_path, \"utt2subdialect\")\n",
    "    dev_text = os.path.join(dev_path, \"text\")\n",
    "    dev_dict = gather_wav_scp(dev_wav_scp, dev_dict)\n",
    "    dev_dict = gether_dialect_info(dev_utts2subdialect, dev_dict)\n",
    "    dev_dict = gather_text_info(dev_text, dev_dict)\n",
    "\n",
    "test_wav_scp = os.path.join(test_dir, \"wav.scp\")\n",
    "test_utts2subdialect = os.path.join(test_dir, \"utt2subdialect\")\n",
    "test_text = os.path.join(test_dir, \"text\")\n",
    "test_dict = gather_wav_scp(test_wav_scp, test_dict)\n",
    "test_dict = gether_dialect_info(test_utts2subdialect, test_dict)\n",
    "test_dict = gather_text_info(test_text, test_dict)\n",
    "\n",
    "clean_train_dict = {}\n",
    "clean_dev_dict = {}\n",
    "clean_test_dict = {}\n",
    "\n",
    "for id in train_dict.keys():\n",
    "    if contains_non_chinese(train_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_train_dict[id] = train_dict[id]\n",
    "for id in dev_dict.keys():\n",
    "    if contains_non_chinese(dev_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_dev_dict[id] = dev_dict[id]\n",
    "for id in test_dict.keys():\n",
    "    if contains_non_chinese(test_dict[id][\"text\"]):\n",
    "        continue\n",
    "    else:\n",
    "        clean_test_dict[id] = test_dict[id]\n",
    "\n",
    "train_dict = clean_train_dict\n",
    "dev_dict = clean_dev_dict\n",
    "test_dict = clean_test_dict\n",
    "\n",
    "\n",
    "print (\"Done!\")\n",
    "print (\"Train dataset size: \", len(train_dict))\n",
    "print (\"Dev dataset size: \", len(dev_dict))\n",
    "print (\"Test dataset size: \", len(test_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each_subdialect   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_subdialect_dir =  os.path.join(output_dir, \"ES\")\n",
    "os.makedirs(each_subdialect_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Make wav.scp and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin train size:  678515 check True\n",
      "Mandarin dev size:  3681 check True\n",
      "Mandarin test size:  4981 check True\n",
      "Done!\n",
      "Beijing train size:  2237 check True\n",
      "Beijing dev size:  31 check True\n",
      "Beijing test size:  265 check True\n",
      "Done!\n",
      "Southwestern train size:  45359 check True\n",
      "Southwestern dev size:  132 check True\n",
      "Southwestern test size:  2684 check True\n",
      "Done!\n",
      "Jiao-Liao train size:  20268 check True\n",
      "Jiao-Liao dev size:  118 check True\n",
      "Jiao-Liao test size:  1443 check True\n",
      "Done!\n",
      "Northeastern train size:  4843 check True\n",
      "Northeastern dev size:  5 check True\n",
      "Northeastern test size:  350 check True\n",
      "Done!\n",
      "Jiang-Huai train size:  27586 check True\n",
      "Jiang-Huai dev size:  105 check True\n",
      "Jiang-Huai test size:  2268 check True\n",
      "Done!\n",
      "Lan-Yin train size:  20549 check True\n",
      "Lan-Yin dev size:  104 check True\n",
      "Lan-Yin test size:  1646 check True\n",
      "Done!\n",
      "Ji-Lu train size:  33861 check True\n",
      "Ji-Lu dev size:  156 check True\n",
      "Ji-Lu test size:  2806 check True\n",
      "Done!\n",
      "Zhongyuan train size:  48590 check True\n",
      "Zhongyuan dev size:  67 check True\n",
      "Zhongyuan test size:  3225 check True\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Mandarin', 'Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "for dialect in subdialect_list:\n",
    "    es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "    os.makedirs(es_subdialect_dir, exist_ok=True)\n",
    "\n",
    "    es_subdialect_train = os.path.join(es_subdialect_dir, \"train\")\n",
    "    es_subdialect_dev = os.path.join(es_subdialect_dir, \"dev\")\n",
    "    es_subdialect_test = os.path.join(es_subdialect_dir, \"test\")\n",
    "\n",
    "    es_subdialect_output = [es_subdialect_train, es_subdialect_dev, es_subdialect_test]\n",
    "    data_dict = [train_dict, dev_dict, test_dict]\n",
    "\n",
    "    for output, dict in zip(es_subdialect_output, data_dict):\n",
    "        os.makedirs(output, exist_ok=True)\n",
    "        wav_scp = os.path.join(output, \"wav.scp\")\n",
    "        text_path = os.path.join(output, \"text\")\n",
    "\n",
    "        with open(wav_scp, 'w') as f:\n",
    "            for id, info in dict.items():\n",
    "                if info[\"dialect\"] == dialect:\n",
    "                    path = info[\"path\"]\n",
    "                    f.write(f\"{id} {path}\\n\")\n",
    "\n",
    "        with open(text_path, 'w') as f:\n",
    "            for id, info in dict.items():\n",
    "                if info[\"dialect\"] == dialect:\n",
    "                    text = info[\"text\"]\n",
    "                    f.write(f\"{id} {text}\\n\")\n",
    "\n",
    "        print (dialect, output.split(\"/\")[-1], \"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))\n",
    "            \n",
    "    print (\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the test to the dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(read_file, writen_file):\n",
    "    # 打开第一个文件以读取数据\n",
    "    with open(read_file, 'r', encoding='utf-8') as f1:\n",
    "        data = f1.read()  # 读取全部内容\n",
    "\n",
    "    # 打开第二个文件以追加数据\n",
    "    with open(writen_file, 'a', encoding='utf-8') as f2:\n",
    "        f2.write(data)  # 将读取的数据追加到文件末尾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing Dev dataset oringinal size:  31 = 31\n",
      "Beijing Test dataset oringinal size:  265 = 265\n",
      "Beijing Dev dataset size:  296 = 296\n",
      "Beijing Test dataset size:  265 = 265\n",
      "\n",
      "\n",
      "Southwestern Dev dataset oringinal size:  132 = 132\n",
      "Southwestern Test dataset oringinal size:  2684 = 2684\n",
      "Southwestern Dev dataset size:  2816 = 2816\n",
      "Southwestern Test dataset size:  2684 = 2684\n",
      "\n",
      "\n",
      "Jiao-Liao Dev dataset oringinal size:  118 = 118\n",
      "Jiao-Liao Test dataset oringinal size:  1443 = 1443\n",
      "Jiao-Liao Dev dataset size:  1561 = 1561\n",
      "Jiao-Liao Test dataset size:  1443 = 1443\n",
      "\n",
      "\n",
      "Northeastern Dev dataset oringinal size:  5 = 5\n",
      "Northeastern Test dataset oringinal size:  350 = 350\n",
      "Northeastern Dev dataset size:  355 = 355\n",
      "Northeastern Test dataset size:  350 = 350\n",
      "\n",
      "\n",
      "Jiang-Huai Dev dataset oringinal size:  105 = 105\n",
      "Jiang-Huai Test dataset oringinal size:  2268 = 2268\n",
      "Jiang-Huai Dev dataset size:  2373 = 2373\n",
      "Jiang-Huai Test dataset size:  2268 = 2268\n",
      "\n",
      "\n",
      "Lan-Yin Dev dataset oringinal size:  104 = 104\n",
      "Lan-Yin Test dataset oringinal size:  1646 = 1646\n",
      "Lan-Yin Dev dataset size:  1750 = 1750\n",
      "Lan-Yin Test dataset size:  1646 = 1646\n",
      "\n",
      "\n",
      "Ji-Lu Dev dataset oringinal size:  156 = 156\n",
      "Ji-Lu Test dataset oringinal size:  2806 = 2806\n",
      "Ji-Lu Dev dataset size:  2962 = 2962\n",
      "Ji-Lu Test dataset size:  2806 = 2806\n",
      "\n",
      "\n",
      "Zhongyuan Dev dataset oringinal size:  67 = 67\n",
      "Zhongyuan Test dataset oringinal size:  3225 = 3225\n",
      "Zhongyuan Dev dataset size:  3292 = 3292\n",
      "Zhongyuan Test dataset size:  3225 = 3225\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "for dialect in subdialect_list:\n",
    "    \n",
    "    es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "    es_subdialect_dev = os.path.join(es_subdialect_dir, \"dev\")\n",
    "    es_subdialect_test = os.path.join(es_subdialect_dir, \"test\")\n",
    "\n",
    "    es_subdialect_dev_wav_scp = os.path.join(es_subdialect_dev, \"wav.scp\")\n",
    "    es_subdialect_dev_text = os.path.join(es_subdialect_dev, \"text\")\n",
    "\n",
    "    es_subdialect_test_wav_scp = os.path.join(es_subdialect_test, \"wav.scp\")\n",
    "    es_subdialect_test_text = os.path.join(es_subdialect_test, \"text\")\n",
    "\n",
    "    print (dialect, \"Dev dataset oringinal size: \", len(open(es_subdialect_dev_wav_scp).readlines()), \"=\", len(open(es_subdialect_dev_text).readlines()))\n",
    "    print (dialect, \"Test dataset oringinal size: \", len(open(es_subdialect_test_wav_scp).readlines()), \"=\", len(open(es_subdialect_test_text).readlines()))\n",
    "\n",
    "    merge_files(es_subdialect_test_wav_scp, es_subdialect_dev_wav_scp)\n",
    "    merge_files(es_subdialect_test_text, es_subdialect_dev_text)\n",
    "\n",
    "    print (dialect, \"Dev dataset size: \", len(open(es_subdialect_dev_wav_scp).readlines()), \"=\", len(open(es_subdialect_dev_text).readlines()))\n",
    "    print (dialect, \"Test dataset size: \", len(open(es_subdialect_test_wav_scp).readlines()),\"=\", len(open(es_subdialect_test_text).readlines()))\n",
    "\n",
    "    print (\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All_subdialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  203293 check True\n",
      "dev size:  15405 check True\n",
      "test size:  14687 check True\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "\n",
    "all_subdialect_dir = os.path.join(output_dir, \"AS\")\n",
    "os.makedirs(all_subdialect_dir, exist_ok=True)\n",
    "all_subdialect_train = os.path.join(all_subdialect_dir, \"train\")\n",
    "WholeDevSet = os.path.join(all_subdialect_dir, \"dev\")\n",
    "WholeTestSet = os.path.join(all_subdialect_dir, \"test\")\n",
    "whole_dataset_output = [all_subdialect_train, WholeDevSet, WholeTestSet]\n",
    "\n",
    "for output in whole_dataset_output:\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    wav_scp = os.path.join(output, \"wav.scp\")\n",
    "    text_path = os.path.join(output, \"text\")\n",
    "\n",
    "    with open(wav_scp, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_wav_scp = os.path.join(es_subdialect_output, \"wav.scp\")\n",
    "            for line in open(es_subdialect_wav_scp):\n",
    "                f.write(line)\n",
    "\n",
    "    with open(text_path, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_text = os.path.join(es_subdialect_output, \"text\")\n",
    "            for line in open(es_subdialect_text):\n",
    "                f.write(line)\n",
    "\n",
    "    print (output.split(\"/\")[-1], \"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  881808 check True\n",
      "dev size:  19086 check True\n",
      "test size:  19668 check True\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Mandarin', 'Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "\n",
    "all_subdialect_dir = os.path.join(output_dir, \"WD\")\n",
    "os.makedirs(all_subdialect_dir, exist_ok=True)\n",
    "all_subdialect_train = os.path.join(all_subdialect_dir, \"train\")\n",
    "WholeDevSet = os.path.join(all_subdialect_dir, \"dev\")\n",
    "WholeTestSet = os.path.join(all_subdialect_dir, \"test\")\n",
    "whole_dataset_output = [all_subdialect_train, WholeDevSet, WholeTestSet]\n",
    "\n",
    "for output in whole_dataset_output:\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    wav_scp = os.path.join(output, \"wav.scp\")\n",
    "    text_path = os.path.join(output, \"text\")\n",
    "\n",
    "    with open(wav_scp, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_wav_scp = os.path.join(es_subdialect_output, \"wav.scp\")\n",
    "            for line in open(es_subdialect_wav_scp):\n",
    "                f.write(line)\n",
    "\n",
    "    with open(text_path, 'w') as f:\n",
    "        for dialect in subdialect_list:\n",
    "            es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "            es_subdialect_output = os.path.join(es_subdialect_dir, output.split(\"/\")[-1])\n",
    "            es_subdialect_text = os.path.join(es_subdialect_output, \"text\")\n",
    "            for line in open(es_subdialect_text):\n",
    "                f.write(line)\n",
    "\n",
    "    print (output.split(\"/\")[-1], \"size: \", len(open(wav_scp).readlines()), \"check\",  len(open(wav_scp).readlines())==len(open(text_path).readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After CNAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(read_file, writen_file):\n",
    "    # 打开第一个文件以读取数据\n",
    "    with open(read_file, 'r', encoding='utf-8') as f1:\n",
    "        data = f1.read()  # 读取全部内容\n",
    "\n",
    "    # 打开第二个文件以追加数据\n",
    "    with open(writen_file, 'a', encoding='utf-8') as f2:\n",
    "        f2.write(data)  # 将读取的数据追加到文件末尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing train dataset size:  2237\n",
      "Southwestern train dataset size:  45359\n",
      "Jiao-Liao train dataset size:  20268\n",
      "Northeastern train dataset size:  4843\n",
      "Jiang-Huai train dataset size:  27586\n",
      "Lan-Yin train dataset size:  20549\n",
      "Ji-Lu train dataset size:  33861\n",
      "Zhongyuan train dataset size:  48590\n",
      "train dataset size:  203293\n",
      "Beijing dev dataset size:  296\n",
      "Southwestern dev dataset size:  2816\n",
      "Jiao-Liao dev dataset size:  1561\n",
      "Northeastern dev dataset size:  355\n",
      "Jiang-Huai dev dataset size:  2373\n",
      "Lan-Yin dev dataset size:  1750\n",
      "Ji-Lu dev dataset size:  2962\n",
      "Zhongyuan dev dataset size:  3292\n",
      "dev dataset size:  15405\n",
      "Beijing test dataset size:  265\n",
      "Southwestern test dataset size:  2684\n",
      "Jiao-Liao test dataset size:  1443\n",
      "Northeastern test dataset size:  350\n",
      "Jiang-Huai test dataset size:  2268\n",
      "Lan-Yin test dataset size:  1646\n",
      "Ji-Lu test dataset size:  2806\n",
      "Zhongyuan test dataset size:  3225\n",
      "test dataset size:  14687\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "section_list = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "all_subdialect_dir = os.path.join(output_dir, \"AS\")\n",
    "each_subdialect_dir = os.path.join(output_dir, \"ES\")\n",
    "\n",
    "\n",
    "for section in section_list:\n",
    "    all_subdialect_section = os.path.join(all_subdialect_dir, section)\n",
    "    all_subdialect_section_audio_datasets = os.path.join(all_subdialect_section, \"audio_datasets.jsonl\")\n",
    "    os.makedirs(all_subdialect_section, exist_ok=True)\n",
    "\n",
    "    for dialect in subdialect_list:\n",
    "        es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "        es_subdialect_section = os.path.join(es_subdialect_dir, section)\n",
    "        es_audio_datasets  = os.path.join(es_subdialect_section, \"audio_datasets.jsonl\")\n",
    "        print (dialect, section, \"dataset size: \", len(open(es_audio_datasets).readlines()))\n",
    "        merge_files(es_audio_datasets, all_subdialect_section_audio_datasets)\n",
    "\n",
    "    print (section, \"dataset size: \", len(open(all_subdialect_section_audio_datasets).readlines()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin train dataset size:  678515\n",
      "Beijing train dataset size:  2237\n",
      "Southwestern train dataset size:  45359\n",
      "Jiao-Liao train dataset size:  20268\n",
      "Northeastern train dataset size:  4843\n",
      "Jiang-Huai train dataset size:  27586\n",
      "Lan-Yin train dataset size:  20549\n",
      "Ji-Lu train dataset size:  33861\n",
      "Zhongyuan train dataset size:  48590\n",
      "train dataset size:  881808\n",
      "Mandarin dev dataset size:  3681\n",
      "Beijing dev dataset size:  296\n",
      "Southwestern dev dataset size:  2816\n",
      "Jiao-Liao dev dataset size:  1561\n",
      "Northeastern dev dataset size:  355\n",
      "Jiang-Huai dev dataset size:  2373\n",
      "Lan-Yin dev dataset size:  1750\n",
      "Ji-Lu dev dataset size:  2962\n",
      "Zhongyuan dev dataset size:  3292\n",
      "dev dataset size:  19086\n",
      "Mandarin test dataset size:  4981\n",
      "Beijing test dataset size:  265\n",
      "Southwestern test dataset size:  2684\n",
      "Jiao-Liao test dataset size:  1443\n",
      "Northeastern test dataset size:  350\n",
      "Jiang-Huai test dataset size:  2268\n",
      "Lan-Yin test dataset size:  1646\n",
      "Ji-Lu test dataset size:  2806\n",
      "Zhongyuan test dataset size:  3225\n",
      "test dataset size:  19668\n"
     ]
    }
   ],
   "source": [
    "subdialect_list=['Mandarin', 'Beijing', 'Southwestern', 'Jiao-Liao', 'Northeastern', 'Jiang-Huai', 'Lan-Yin', 'Ji-Lu', 'Zhongyuan']\n",
    "section_list = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "whole_dir = os.path.join(output_dir, \"WD\")\n",
    "each_subdialect_dir = os.path.join(output_dir, \"ES\")\n",
    "\n",
    "for section in section_list:\n",
    "    whole_section = os.path.join(whole_dir, section)\n",
    "    whole_audio_datasets = os.path.join(whole_section, \"audio_datasets.jsonl\")\n",
    "    os.makedirs(whole_section, exist_ok=True)\n",
    "\n",
    "    for dialect in subdialect_list:\n",
    "        es_subdialect_dir = os.path.join(each_subdialect_dir, dialect)\n",
    "        es_subdialect_section = os.path.join(es_subdialect_dir, section)\n",
    "        es_audio_datasets  = os.path.join(es_subdialect_section, \"audio_datasets.jsonl\")\n",
    "        print (dialect, section, \"dataset size: \", len(open(es_audio_datasets).readlines()))\n",
    "        merge_files(es_audio_datasets, whole_audio_datasets)\n",
    "\n",
    "    print (section, \"dataset size: \", len(open(whole_audio_datasets).readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funasr2024_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
